{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Data from Amazon\n",
    "---\n",
    "In this notebook, we collect the necessary data by scraping it directly from Amazon.\n",
    "\n",
    "The dataset we want:\n",
    "\n",
    "| ID | Review Score | Sales Rank | Category    | Title | Author | Date    | Visual Features     |\n",
    "| -- | ------------ | ---------- | ----------- | ----- | ------ | ------- | ------------------- |\n",
    "\n",
    "The dataset we have, as downloaded from [here](https://github.com/uchidalab/book-dataset):\n",
    "\n",
    "| ID | Filename | Image URL | Title | Author | Category ID | Category |\n",
    "| -- | -------- | --------- | ----- | ------ | ----------- | -------- |\n",
    "\n",
    "The `ID` column in the data can be used to access the webpage of each book, by connecting to https://www.amazon.com/dp/book-id. This allows us to scrape any data that is missing directly from Amazon.\n",
    "\n",
    "We already have the Title, Author and Category of each book ready to be used.\n",
    "\n",
    "For everything else, there's ~~Mastercard~~ BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To request data from Amazon\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# To open image links\n",
    "import urllib\n",
    "\n",
    "# To process data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# To extract information from weirdly formatted Amazon info\n",
    "import re\n",
    "\n",
    "# To create random delays to trick the Amazon bot detector\n",
    "from time import sleep\n",
    "import random\n",
    "\n",
    "# To rotate IPs while scraping | WARNING: Don't forget to run `tor` in the terminal before executing this cell\n",
    "from torrequest import TorRequest\n",
    "tor = TorRequest(password='i<3cs401')\n",
    "\n",
    "# To rotate user-agents while scraping\n",
    "from fake_useragent import UserAgent\n",
    "user_agent = UserAgent()\n",
    "\n",
    "# To read data\n",
    "import csv\n",
    "\n",
    "# For file operations\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# For parallelizing tasks \n",
    "import dask.dataframe as dd\n",
    "import dask.multiprocessing\n",
    "from dask import compute, delayed\n",
    "\n",
    "# To print an image in the notebook programmatically\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Set data directories\n",
    "ORIGINAL_DATA_DIR = 'Original Data/'\n",
    "COLLECTED_DATA_DIR = 'Collected Data/'\n",
    "IMAGE_DIR = COLLECTED_DATA_DIR + 'Cover Images/'\n",
    "HTML_DIR = '/Users/dogatekin/Data/HTML Files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Original Data\n",
    "---\n",
    "\n",
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Image URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Category ID</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>761183272</td>\n",
       "      <td>0761183272.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61Y5cOdH...</td>\n",
       "      <td>Mom's Family Wall Calendar 2016</td>\n",
       "      <td>Sandra Boynton</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1623439671</td>\n",
       "      <td>1623439671.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61t-hrSw...</td>\n",
       "      <td>Doug the Pug 2016 Wall Calendar</td>\n",
       "      <td>Doug the Pug</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00O80WC6I</td>\n",
       "      <td>B00O80WC6I.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41X-KQqs...</td>\n",
       "      <td>Moleskine 2016 Weekly Notebook, 12M, Large, Bl...</td>\n",
       "      <td>Moleskine</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>761182187</td>\n",
       "      <td>0761182187.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61j-4gxJ...</td>\n",
       "      <td>365 Cats Color Page-A-Day Calendar 2016</td>\n",
       "      <td>Workman Publishing</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1578052084</td>\n",
       "      <td>1578052084.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51Ry4Tsq...</td>\n",
       "      <td>Sierra Club Engagement Calendar 2016</td>\n",
       "      <td>Sierra Club</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID        Filename  \\\n",
       "0   761183272  0761183272.jpg   \n",
       "1  1623439671  1623439671.jpg   \n",
       "2  B00O80WC6I  B00O80WC6I.jpg   \n",
       "3   761182187  0761182187.jpg   \n",
       "4  1578052084  1578052084.jpg   \n",
       "\n",
       "                                           Image URL  \\\n",
       "0  http://ecx.images-amazon.com/images/I/61Y5cOdH...   \n",
       "1  http://ecx.images-amazon.com/images/I/61t-hrSw...   \n",
       "2  http://ecx.images-amazon.com/images/I/41X-KQqs...   \n",
       "3  http://ecx.images-amazon.com/images/I/61j-4gxJ...   \n",
       "4  http://ecx.images-amazon.com/images/I/51Ry4Tsq...   \n",
       "\n",
       "                                               Title              Author  \\\n",
       "0                    Mom's Family Wall Calendar 2016      Sandra Boynton   \n",
       "1                    Doug the Pug 2016 Wall Calendar        Doug the Pug   \n",
       "2  Moleskine 2016 Weekly Notebook, 12M, Large, Bl...           Moleskine   \n",
       "3            365 Cats Color Page-A-Day Calendar 2016  Workman Publishing   \n",
       "4               Sierra Club Engagement Calendar 2016         Sierra Club   \n",
       "\n",
       "   Category ID   Category  \n",
       "0            3  Calendars  \n",
       "1            3  Calendars  \n",
       "2            3  Calendars  \n",
       "3            3  Calendars  \n",
       "4            3  Calendars  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_names = ['ID', 'Filename', 'Image URL', 'Title', 'Author', 'Category ID', 'Category']\n",
    "\n",
    "books = pd.read_csv(ORIGINAL_DATA_DIR + 'book32-listing.csv', encoding='latin1', header=None, names=header_names)\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calendars\n",
      "Comics & Graphic Novels\n",
      "Test Preparation\n",
      "Mystery, Thriller & Suspense\n",
      "Science Fiction & Fantasy\n",
      "Romance\n",
      "Humor & Entertainment\n",
      "Literature & Fiction\n",
      "Gay & Lesbian\n",
      "Engineering & Transportation\n",
      "Cookbooks, Food & Wine\n",
      "Crafts, Hobbies & Home\n",
      "Arts & Photography\n",
      "Education & Teaching\n",
      "Parenting & Relationships\n",
      "Self-Help\n",
      "Computers & Technology\n",
      "Medical Books\n",
      "Science & Math\n",
      "Health, Fitness & Dieting\n",
      "Business & Money\n",
      "Law\n",
      "Biographies & Memoirs\n",
      "History\n",
      "Politics & Social Sciences\n",
      "Reference\n",
      "Christian Books & Bibles\n",
      "Religion & Spirituality\n",
      "Sports & Outdoors\n",
      "Teen & Young Adult\n",
      "Children's Books\n",
      "Travel\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(books['Category'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want the Children's Books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Image URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>545790352</td>\n",
       "      <td>0545790352.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51MIi4p2...</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone: The Ill...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1419717014</td>\n",
       "      <td>1419717014.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61YgGsg-...</td>\n",
       "      <td>Diary of a Wimpy Kid: Old School</td>\n",
       "      <td>Jeff Kinney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1423160916</td>\n",
       "      <td>1423160916.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/611CmvkL...</td>\n",
       "      <td>Magnus Chase and the Gods of Asgard, Book 1: T...</td>\n",
       "      <td>Rick Riordan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1476789886</td>\n",
       "      <td>1476789886.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51KqU7Dw...</td>\n",
       "      <td>Rush Revere and the Star-Spangled Banner</td>\n",
       "      <td>Rush Limbaugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1338029991</td>\n",
       "      <td>1338029991.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61kvq74k...</td>\n",
       "      <td>Harry Potter Coloring Book</td>\n",
       "      <td>Scholastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID        Filename  \\\n",
       "0   545790352  0545790352.jpg   \n",
       "1  1419717014  1419717014.jpg   \n",
       "2  1423160916  1423160916.jpg   \n",
       "3  1476789886  1476789886.jpg   \n",
       "4  1338029991  1338029991.jpg   \n",
       "\n",
       "                                           Image URL  \\\n",
       "0  http://ecx.images-amazon.com/images/I/51MIi4p2...   \n",
       "1  http://ecx.images-amazon.com/images/I/61YgGsg-...   \n",
       "2  http://ecx.images-amazon.com/images/I/611CmvkL...   \n",
       "3  http://ecx.images-amazon.com/images/I/51KqU7Dw...   \n",
       "4  http://ecx.images-amazon.com/images/I/61kvq74k...   \n",
       "\n",
       "                                               Title         Author  \n",
       "0  Harry Potter and the Sorcerer's Stone: The Ill...   J.K. Rowling  \n",
       "1                   Diary of a Wimpy Kid: Old School    Jeff Kinney  \n",
       "2  Magnus Chase and the Gods of Asgard, Book 1: T...   Rick Riordan  \n",
       "3           Rush Revere and the Star-Spangled Banner  Rush Limbaugh  \n",
       "4                         Harry Potter Coloring Book     Scholastic  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books = books[books['Category'] == \"Children's Books\"].reset_index(drop=True)\n",
    "# We don't need the Category or Category ID columns anymore\n",
    "books.drop(columns=['Category ID', 'Category'], inplace=True)\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many books we have left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13605"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's fix the IDs in the dataset. For some reason, the ID column has the leading 0s removed (normally all of them should be 10 characters long), which makes the webpages inaccessible. The filename column has the correct IDs with the correct number of leading 0s. So let's use the Filename column as the new ID column, we can add the `.jpg` extension later when downloading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Image URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0545790352</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51MIi4p2...</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone: The Ill...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1419717014</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61YgGsg-...</td>\n",
       "      <td>Diary of a Wimpy Kid: Old School</td>\n",
       "      <td>Jeff Kinney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1423160916</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/611CmvkL...</td>\n",
       "      <td>Magnus Chase and the Gods of Asgard, Book 1: T...</td>\n",
       "      <td>Rick Riordan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1476789886</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51KqU7Dw...</td>\n",
       "      <td>Rush Revere and the Star-Spangled Banner</td>\n",
       "      <td>Rush Limbaugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1338029991</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61kvq74k...</td>\n",
       "      <td>Harry Potter Coloring Book</td>\n",
       "      <td>Scholastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                          Image URL  \\\n",
       "0  0545790352  http://ecx.images-amazon.com/images/I/51MIi4p2...   \n",
       "1  1419717014  http://ecx.images-amazon.com/images/I/61YgGsg-...   \n",
       "2  1423160916  http://ecx.images-amazon.com/images/I/611CmvkL...   \n",
       "3  1476789886  http://ecx.images-amazon.com/images/I/51KqU7Dw...   \n",
       "4  1338029991  http://ecx.images-amazon.com/images/I/61kvq74k...   \n",
       "\n",
       "                                               Title         Author  \n",
       "0  Harry Potter and the Sorcerer's Stone: The Ill...   J.K. Rowling  \n",
       "1                   Diary of a Wimpy Kid: Old School    Jeff Kinney  \n",
       "2  Magnus Chase and the Gods of Asgard, Book 1: T...   Rick Riordan  \n",
       "3           Rush Revere and the Star-Spangled Banner  Rush Limbaugh  \n",
       "4                         Harry Potter Coloring Book     Scholastic  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books['ID'] = books['Filename'].apply(lambda row: re.findall(u'(.*).jpg', row)[0])\n",
    "books.drop(columns='Filename', inplace=True)\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this small version of the data for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.drop(columns=['Image URL']).to_csv(COLLECTED_DATA_DIR + 'orig_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping New Data\n",
    "---\n",
    "\n",
    "The columns we need to scrape are: `Review Score`, `Sales Rank` and `Date`. We also need to download the images from the URLs so that we can extract visual features from them, completing our dataset. Just in case we need some other information in the future from the webpages, we will also save the raw HTML files so we don't have to scrape them from Amazon again.\n",
    "\n",
    "First we will demonstrate the scraping process for each column on an arbitrary example, then we will combine these in a function and scrape the information for all the books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                                  0545790352\n",
       "Image URL    http://ecx.images-amazon.com/images/I/51MIi4p2...\n",
       "Title        Harry Potter and the Sorcerer's Stone: The Ill...\n",
       "Author                                            J.K. Rowling\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_book = books.iloc[0]\n",
    "example_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Amazon\n",
    "\n",
    "This step is trickier than it sounds. Sending many requests to Amazon servers in quick succession always leads to Captcha pages that check if the request came from a human. In this case, it is indeed not coming from a human so we need to be smarter. We use Tor requests to be able to change our IP at any time and also rotate the User Agent we use to send the request.\n",
    "\n",
    "We also noticed that at least one Tor IP was unable to connect to the servers, so we try the initial request many times with different IPs and user agents until we get a response without any connection errors or getting caught by the bot detector. When a request is successful, we keep using the found IP-agent pair until it fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(book_id, agent=user_agent.random, max_tries=10, wait=600):\n",
    "    for i in range(max_tries):\n",
    "        try:\n",
    "            # Creating random delays before requests helps to avoid detection\n",
    "#             sleep(random.randint(1, 2))\n",
    "            \n",
    "            # Try to connect\n",
    "            response = tor.get('https://www.amazon.com/dp/' + book_id, headers={'User-Agent': agent})\n",
    "            status = response.status_code\n",
    "            \n",
    "            # Check if page still exists\n",
    "            if(status != 200):\n",
    "                return status, None, agent, None\n",
    "            \n",
    "            # Make soup if we didn't get any errors\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "            \n",
    "            # If we get redirected to a Captcha page raise error to try again\n",
    "            if(soup.title.string == 'Robot Check'):\n",
    "                raise ConnectionError\n",
    "            \n",
    "            # If we successfully reach the webpage, return the soup, successful agent and raw HTML\n",
    "            return status, soup, agent, response.text\n",
    "        \n",
    "        except ConnectionError:\n",
    "            # If something is wrong with the IP, get a new IP and user agent and try again\n",
    "            tor.reset_identity()\n",
    "            agent = user_agent.random\n",
    "            if(i == int(max_tries / 2)):\n",
    "                print(f'Half of {max_tries} trials failed for book ID {book_id}, waiting for {int(wait/60)} mins.      ', end='\\r')\n",
    "                sleep(wait)\n",
    "            else:\n",
    "                print(f'Trial {i+1} failed to connect for book ID {book_id}, resetting IP and trying again.', end='\\r')\n",
    "    \n",
    "    raise ConnectionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it on our example book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Harry Potter and the Sorcerer's Stone: The Illustrated Edition (Harry Potter, Book 1): J.K. Rowling, Jim Kay: 9780545790352: Amazon.com: Books\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, soup, _, _ = connect(example_book['ID'])\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales Rank and Date\n",
    "\n",
    "We can get both of these from the product details table on the webpage, which is in a table conveniently named `productDetailsTable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<b>Age Range:</b>,\n",
       " <b>Grade Level:</b>,\n",
       " <b>Series:</b>,\n",
       " <b>Hardcover:</b>,\n",
       " <b>Publisher:</b>,\n",
       " <b>Language:</b>,\n",
       " <b>ISBN-10:</b>,\n",
       " <b>ISBN-13:</b>,\n",
       " <b>\n",
       "     Product Dimensions: \n",
       "     </b>,\n",
       " <b>Shipping Weight:</b>,\n",
       " <b>Average Customer Review:</b>,\n",
       " <b>Amazon Best Sellers Rank:</b>,\n",
       " <b><a href=\"https://www.amazon.com/gp/bestsellers/books/3153/ref=pd_zg_hrsr_books_1_5_last/134-2712085-9861750\">Friendship</a></b>,\n",
       " <b><a href=\"https://www.amazon.com/gp/bestsellers/books/2967/ref=pd_zg_hrsr_books_2_3_last/134-2712085-9861750\">Action &amp; Adventure</a></b>,\n",
       " <b><a href=\"https://www.amazon.com/gp/bestsellers/books/3017/ref=pd_zg_hrsr_books_3_4_last/134-2712085-9861750\">Fantasy &amp; Magic</a></b>]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('#productDetailsTable li b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use regex to extract the info we need from the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Rank: 124\n",
      "Date: October 6, 2015\n"
     ]
    }
   ],
   "source": [
    "for li in soup.select('#productDetailsTable li'):\n",
    "    # We only need two of the list items\n",
    "    if(li.b.string == 'Amazon Best Sellers Rank:'):\n",
    "        # The rank is given in the format #1,234,567\n",
    "        sales_rank = re.findall(u'#([\\d,]+)', li.b.nextSibling)[0]\n",
    "    elif(li.b.string == 'Publisher:'):\n",
    "        # The date is in the last set of parantheses\n",
    "        date = re.findall(u'\\(([^\\(\\)]*)\\)$', li.b.nextSibling)[0]\n",
    "        \n",
    "print(f'Sales Rank: {sales_rank}\\nDate: {date}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn it into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rank_date(soup):\n",
    "    # Initial values to return if cannot be scraped\n",
    "    sales_rank = date = None\n",
    "    \n",
    "    for li in soup.select('#productDetailsTable li'):\n",
    "        try:\n",
    "            if(li.b.string == 'Amazon Best Sellers Rank:'):\n",
    "                sales_rank = re.findall(u'#([\\d,]+)', li.b.nextSibling)[0]  # Format: #1,234,567\n",
    "                sales_rank = int(sales_rank.replace(',',''))  # Remove the commas and convert to integer\n",
    "        except:\n",
    "            sales_rank = None  # couldn't scrape\n",
    "            \n",
    "        try:\n",
    "            if(li.b.string == 'Publisher:'):\n",
    "                date = re.findall(u'\\(([^\\(\\)]*)\\)$', li.b.nextSibling)[0]  # Format: Inside last parantheses\n",
    "        except:\n",
    "            date = None  # couldn't scrape\n",
    "                \n",
    "    return sales_rank, date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try on example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 'October 6, 2015')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_rank_date(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Score\n",
    "\n",
    "You might have noticed there is also an item called `Average Customer Review` in the table we just used to extract the Rank and Date. Inside that item, all the review scores are found in a table with the id `histogramTable`, that gives the percentages of users for each score from 1 to 5 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 star87%4 star8%3 star2%2 star1%1 star2%'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = soup.select('#histogramTable')[0].text\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formatting is not great, but it's nothing we can't fix by using a simple regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5', '87'), ('4', '8'), ('3', '2'), ('2', '1'), ('1', '2')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = re.findall(u'(\\d) star(\\d+)%', reviews)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted average of these scores is our final Review Score for the given book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.77"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = 0\n",
    "for pair in reviews:\n",
    "    score += int(pair[0]) * int(pair[1])/100  # weights are percentages\n",
    "\n",
    "round(score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(soup):\n",
    "    # Initial value to return if cannot be scraped\n",
    "    score = None\n",
    "    \n",
    "    try:\n",
    "        reviews = soup.select('#histogramTable')[0].text\n",
    "        reviews = re.findall(u'(\\d) star(\\d+)%', reviews)\n",
    "\n",
    "        score = 0\n",
    "        for pair in reviews:\n",
    "            score += int(pair[0]) * int(pair[1])/100  # weights are percentages\n",
    "\n",
    "        score = round(score, 3)\n",
    "    except:\n",
    "        score = None  # couldn't scrape\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try on example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.77"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_score(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cover Image\n",
    "\n",
    "The image URL of each book is available in the original dataset, let's make a HashMap of `ID:URL` pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0545790352': 'http://ecx.images-amazon.com/images/I/51MIi4p2YyL.jpg',\n",
       " '1419717014': 'http://ecx.images-amazon.com/images/I/61YgGsg-k-L.jpg',\n",
       " '1423160916': 'http://ecx.images-amazon.com/images/I/611CmvkLO4L.jpg',\n",
       " '1476789886': 'http://ecx.images-amazon.com/images/I/51KqU7Dw9SL.jpg',\n",
       " '1338029991': 'http://ecx.images-amazon.com/images/I/61kvq74kVSL.jpg'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = books[['ID', 'Image URL']].set_index('ID').to_dict()['Image URL']\n",
    "\n",
    "# Show random 5 mappings\n",
    "dict(list(urls.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it on our example book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://ecx.images-amazon.com/images/I/51MIi4p2YyL.jpg'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_url = urls[example_book['ID']]\n",
    "example_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![Example Image](http://ecx.images-amazon.com/images/I/51MIi4p2YyL.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(f'![Example Image]({example_url})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn it into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(book_id):\n",
    "    url = urls[book_id]\n",
    "    filename = book_id + '.jpg'\n",
    "    \n",
    "    # Download only if not already downloaded\n",
    "    if not os.path.isfile(IMAGE_DIR + filename):\n",
    "        downloaded_img = urllib.request.urlopen(url)\n",
    "        f = open(IMAGE_DIR + filename, mode='wb')\n",
    "        f.write(downloaded_img.read())\n",
    "        downloaded_img.close()\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw HTML\n",
    "\n",
    "Save the raw HTML files so we don't have to scrape them from Amazon again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(book_id, html_text):\n",
    "    filename = book_id + '.html'\n",
    "    \n",
    "    # Save only if not already saved\n",
    "    if not os.path.isfile(HTML_DIR + filename):\n",
    "        html_file = open(HTML_DIR + filename,\"w\")\n",
    "        html_file.write(html_text)\n",
    "        html_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it together\n",
    "\n",
    "Let's bring all of the functions we created together under one function that will connect to the webpage, scrape all the necessary info, download the cover image and save the HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_info(book_id, agent=user_agent.random):\n",
    "    try:\n",
    "        # Connect to Amazon, keep track of agent\n",
    "        status, soup, current_agent, raw_html = connect(book_id, agent)\n",
    "\n",
    "        if(status == 200):\n",
    "            # Save the HTML file\n",
    "            save_html(book_id, raw_html)\n",
    "\n",
    "            # Get sales rank and date\n",
    "            sales_rank, date = extract_rank_date(soup)\n",
    "\n",
    "            # Get average review score\n",
    "            score = extract_score(soup)\n",
    "\n",
    "            # Download cover image\n",
    "            download_image(book_id)\n",
    "        else:\n",
    "            # Log the error\n",
    "            sales_rank = date = score = f'Error {status}'\n",
    "        \n",
    "    except ConnectionError:\n",
    "        current_agent = agent\n",
    "        sales_rank = date = score = None\n",
    "        \n",
    "    return current_agent, book_id, sales_rank, date, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a final test on the example book we used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0545790352', 132, 'October 6, 2015', 4.77)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped = scrape_info(example_book['ID'])\n",
    "scraped[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing the dataset\n",
    "---\n",
    "\n",
    "To be able to stop and continue at will, we will write the scraped info to a csv file as we go along, and simultaneously download cover images. Let's initialize this file with a meaningful header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(COLLECTED_DATA_DIR + 'scraped.csv', 'a') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['ID', 'Sales Rank', 'Date', 'Review Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go through the dataset, starting scraping from where we last left off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping finished, enjoy your 13605 books!                                       \n"
     ]
    }
   ],
   "source": [
    "with open(COLLECTED_DATA_DIR + 'scraped.csv', 'a+') as file:\n",
    "    reader = csv.reader(file)\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Look at the last scraped book to continue from the next one in the dataset\n",
    "    file.seek(0)\n",
    "    last_scraped = next(reversed(list(reader)))[0]\n",
    "    \n",
    "    if(last_scraped == 'ID'):\n",
    "        # Nothing was scraped yet, start from the beginning\n",
    "        index = 0\n",
    "    else:\n",
    "        # At least one book was scraped, find the index of the last scraped book and start from the next one\n",
    "        last_scraped_index = books.index[books['ID'] == last_scraped].tolist()[0]\n",
    "        index = last_scraped_index + 1\n",
    "     \n",
    "    try:\n",
    "        agent = user_agent.random\n",
    "        count = 0    \n",
    "        while(index < books.shape[0]):\n",
    "            current_id = books.iloc[index]['ID']\n",
    "            scraped = scrape_info(current_id, agent)\n",
    "\n",
    "            # Keep track of agent\n",
    "            agent = scraped[0]\n",
    "\n",
    "            writer.writerow(scraped[1:])\n",
    "            file.flush()\n",
    "\n",
    "            index += 1\n",
    "            count += 1\n",
    "\n",
    "            # Clean the previous line while printing info about scraping progress\n",
    "            print(f'Number of scraped books: {count}                                                     ', end='\\r')\n",
    "            \n",
    "        print(f'Scraping finished, enjoy your {books.shape[0]} books!')\n",
    "    except KeyboardInterrupt:\n",
    "        print(f'Scraping stopped by manual interruption. Check the last downloaded book cover image and the last row of the CSV file to make sure there were no corruptions. Total number of books scraped until interruption: {count}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing the Missing Parts\n",
    "---\n",
    "\n",
    "Because there were some imperfections in the scraping, here we collect whatever we missed in the first pass-through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sales Rank</th>\n",
       "      <th>Date</th>\n",
       "      <th>Review Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0545790352</td>\n",
       "      <td>118</td>\n",
       "      <td>October 6, 2015</td>\n",
       "      <td>4.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1419717014</td>\n",
       "      <td>399</td>\n",
       "      <td>November 3, 2015</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1423160916</td>\n",
       "      <td>9637</td>\n",
       "      <td>October 6, 2015</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1476789886</td>\n",
       "      <td>5439</td>\n",
       "      <td>October 27, 2015</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1338029991</td>\n",
       "      <td>196</td>\n",
       "      <td>November 10, 2015</td>\n",
       "      <td>4.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID Sales Rank               Date Review Score\n",
       "0  0545790352        118    October 6, 2015         4.77\n",
       "1  1419717014        399   November 3, 2015          4.8\n",
       "2  1423160916       9637    October 6, 2015          4.6\n",
       "3  1476789886       5439   October 27, 2015          4.9\n",
       "4  1338029991        196  November 10, 2015         4.61"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(13605, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scraped = pd.read_csv('Collected Data/scraped.csv')\n",
    "display(scraped.head(), scraped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The books where the scraping failed were written to the file with three `None`s. So those rows are the ones we should go over again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sales Rank</th>\n",
       "      <th>Date</th>\n",
       "      <th>Review Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>0062233009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>0753456095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>0439903742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>0399256059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>1770496459</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID Sales Rank Date Review Score\n",
       "835  0062233009        NaN  NaN          NaN\n",
       "839  0753456095        NaN  NaN          NaN\n",
       "843  0439903742        NaN  NaN          NaN\n",
       "844  0399256059        NaN  NaN          NaN\n",
       "847  1770496459        NaN  NaN          NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(946, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "missed = scraped[scraped.drop(columns='ID').isna().all(1)]\n",
    "display(missed.head(), missed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write functions to do this and for now save them in another folder to be safe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISSED_HTML_DIR = '/Users/dogatekin/Data/Missed HTML Files/'\n",
    "\n",
    "def missed_save_html(book_id, html_text, path=MISSED_HTML_DIR):\n",
    "    filename = book_id + '.html'\n",
    "    \n",
    "    # Save only if not already saved\n",
    "    if not os.path.isfile(path + filename):\n",
    "        html_file = open(path + filename,\"w\")\n",
    "        html_file.write(html_text)\n",
    "        html_file.close()\n",
    "\n",
    "def collect_missed(book_id, path=MISSED_HTML_DIR):\n",
    "    filename = book_id + '.html'\n",
    "    \n",
    "    if not os.path.isfile(path + filename):\n",
    "        status, _, _, html_text = connect(book_id)\n",
    "        \n",
    "        if status == 200:\n",
    "            missed_save_html(book_id, html_text, path)\n",
    "        \n",
    "        return book_id, status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the scraping and see if we had any errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: 944\n",
      "Encountered errors on: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([('1508457123', 404), ('1452112657', 404)], dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = np.array(compute(*[delayed(collect_missed)(ID) for ID in missed['ID']], scheduler='processes'))\n",
    "\n",
    "print(f'Successfully downloaded: {len(errors[errors == None])}')\n",
    "print(f'Encountered errors on: {(len(errors[errors != None]))}')\n",
    "errors[errors != None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were two unreachable books among the ones we missed (pages that give 404 errors cannot be reached by browsers either), but we successfully downloaded the HTML files of all the other missing books. We now add these to the previously collected files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13345"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for file in os.listdir(MISSED_HTML_DIR):\n",
    "    shutil.copy(MISSED_HTML_DIR + file, HTML_DIR)\n",
    "    \n",
    "len(os.listdir(HTML_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still missing a few HTML files. That is because we didn't start collecting the HTML files until some time after we started the scraping process. Let's quickly complete those as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded HTML files: 13535\n",
      "Encountered errors on: 70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([('1507745923', 404), ('1423160657', 404), ('1508457123', 404),\n",
       "       ('1452112657', 404), ('151206212X', 404), ('0375848134', 404),\n",
       "       ('1846432065', 404), ('1494431726', 404), ('1500600016', 404),\n",
       "       ('1508743061', 404), ('1512373753', 404), ('1511822074', 404),\n",
       "       ('1512212512', 404), ('150310902X', 404), ('151413151X', 404),\n",
       "       ('1511878282', 404), ('1497403693', 404), ('1505211905', 404),\n",
       "       ('1505488818', 404), ('1505598257', 404), ('1505532914', 404),\n",
       "       ('1505613361', 404), ('1503322971', 404), ('1505808871', 404),\n",
       "       ('1514369303', 404), ('1505630592', 404), ('1503324354', 404),\n",
       "       ('B0144KN6PC', 404), ('B00XLZW19O', 404), ('B00XLX3W9O', 404),\n",
       "       ('B00PA1HRSW', 404), ('B00R5K8JPG', 404), ('B00PDDRNAO', 404),\n",
       "       ('B00P8HRS8W', 404), ('B00NMNQNTY', 404), ('B00QZ86LRW', 404),\n",
       "       ('B00PO53R8I', 404), ('B00PSOCBXM', 404), ('B00SI7QN2G', 404),\n",
       "       ('B00SI7QG0K', 404), ('B00KUV5FY0', 404), ('B00PG06PCQ', 404),\n",
       "       ('B00Q8U5H6S', 404), ('B00PUKZGDQ', 404), ('B00RD8RCRG', 404),\n",
       "       ('B00R8P7H5Q', 404), ('B00PCNCYRM', 404), ('B00PKN8NXI', 404),\n",
       "       ('1517385970', 404), ('1512383171', 404), ('B011M9IJ5K', 404),\n",
       "       ('1517386055', 404), ('B016QAU50C', 404), ('0938497324', 404),\n",
       "       ('B00XIJNOQC', 404), ('1511990260', 404), ('151211071X', 404),\n",
       "       ('1505644542', 404), ('151199021X', 404), ('1505537223', 404),\n",
       "       ('1517389704', 404), ('1512185566', 404), ('1610183452', 404),\n",
       "       ('1599908581', 404), ('1619633027', 404), ('1579128297', 404),\n",
       "       ('0062012452', 404), ('0061686212', 404), ('1448827477', 404),\n",
       "       ('1607301288', 404)], dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = np.array(compute(*[delayed(collect_missed)(ID, HTML_DIR) for ID in books['ID']], scheduler='processes'))\n",
    "\n",
    "print(f'Downloaded HTML files: {len(errors[errors == None])}')\n",
    "print(f'Encountered errors on: {(len(errors[errors != None]))}')\n",
    "errors[errors != None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70 of the pages in the data are unreachable, but we have everything else. Now that we have the HTML files of all the books, it is easy to extract the information locally without worrying about being blocked by Amazon. In fact, let's quickly rewrite our extraction functions to work on local HTML data while also cleaning them and improving them. In fact, let's add a review count extractor as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'85,536'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(open(HTML_DIR + '0807588997.html'), 'lxml')\n",
    "re.findall(r'#([\\d,]+) in Books \\(', soup.select('#SalesRank')[0].find('td', {'class':'value'}).text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-44-e2aa78b590ac>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-44-e2aa78b590ac>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for i in range(10) if i > 5:\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for i in range(10) if i > 5:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sales_rank(soup):\n",
    "    if soup.select('#SalesRank'):\n",
    "        if soup.select('#SalesRank')[0].b:\n",
    "            if re.findall(r'#([\\d,]+) in Books \\(', soup.select('#SalesRank')[0].b.nextSibling):\n",
    "                sales_rank = re.findall(r'#([\\d,]+) in Books \\(', soup.select('#SalesRank')[0].b.nextSibling)[0]\n",
    "                sales_rank = int(sales_rank.replace(',',''))\n",
    "            else:\n",
    "                sales_rank = None\n",
    "        elif soup.select('#SalesRank')[0].find('td', {'class':'value'}):\n",
    "            sales_rank = re.findall(r'#([\\d,]+) in Books \\(', soup.select('#SalesRank')[0].find('td', {'class':'value'}).text)[0]\n",
    "            sales_rank = int(sales_rank.replace(',',''))\n",
    "        else:\n",
    "            sales_rank = None\n",
    "    elif soup.select('#productDetails_detailBullets_sections1'):\n",
    "        if re.findall(r'#([\\d,]+) in Books \\(', soup.select('#productDetails_detailBullets_sections1')[0].text):\n",
    "            sales_rank = re.findall(r'#([\\d,]+) in Books \\(', soup.select('#productDetails_detailBullets_sections1')[0].text)[0]\n",
    "            sales_rank = int(sales_rank.replace(',',''))\n",
    "        else:\n",
    "            sales_rank = None\n",
    "    else:\n",
    "        # Sales rank is either not given, or it is not given for the Books category\n",
    "        sales_rank = None\n",
    "\n",
    "    return sales_rank\n",
    "\n",
    "def extract_date(soup):\n",
    "#     if re.findall(r' (.*)', soup.select('#title')[0].findAll('span')[-1].text):\n",
    "#         date = re.findall(r' (.*)', soup.select('#title')[0].findAll('span')[-1].text)[0]\n",
    "#     else:\n",
    "    for li in soup.select('#productDetailsTable li'):\n",
    "        if(li.b and li.b.string == 'Publisher:' and re.findall(u'\\(([^\\(\\)]*)\\)$', li.b.nextSibling)):\n",
    "            date = re.findall(u'\\(([^\\(\\)]*)\\)$', li.b.nextSibling)[0]\n",
    "            break\n",
    "    else:\n",
    "        # Date could not be found\n",
    "        date = None\n",
    "\n",
    "    return date\n",
    "\n",
    "def extract_score(soup):\n",
    "    if soup.select('#histogramTable'):\n",
    "        reviews = soup.select('#histogramTable')[0].text\n",
    "        reviews = re.findall(u'(\\d) star(\\d+)%', reviews)\n",
    "\n",
    "        score = 0\n",
    "        for pair in reviews:\n",
    "            score += int(pair[0]) * int(pair[1])/100  # weights are percentages\n",
    "\n",
    "        score = round(score, 3)\n",
    "    else:\n",
    "        # Score could not be found\n",
    "        score = None\n",
    "        \n",
    "    return score\n",
    "\n",
    "def extract_review_count(soup):\n",
    "    if soup.select('#acrCustomerReviewText'):\n",
    "        review_count = re.split(' ', soup.select('#acrCustomerReviewText')[0].string)[0]\n",
    "        review_count = int(review_count.replace(',',''))\n",
    "    elif soup.select('#acrCustomerWriteReviewText')[0].string == 'Be the first to review this item':\n",
    "        review_count = 0\n",
    "    else:\n",
    "        # Review count cannot be found\n",
    "        review_count = None\n",
    "    \n",
    "    return review_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one function to bring them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_html(book_id, path=HTML_DIR):    \n",
    "    filename = book_id + '.html'\n",
    "    \n",
    "    soup = BeautifulSoup(open(path + filename), \"lxml\")\n",
    "    \n",
    "    try:\n",
    "        sales_rank = extract_sales_rank(soup)\n",
    "        date = extract_date(soup)\n",
    "        score = extract_score(soup)\n",
    "        review_count = extract_review_count(soup)\n",
    "\n",
    "        # Download the image as well if it is not already downloaded\n",
    "        download_image(book_id)\n",
    "\n",
    "        return book_id, sales_rank, date, score, review_count\n",
    "    except Exception as e:\n",
    "        print(f'Error on: https://www.amazon.com/dp/{book_id}')\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the efficiency of parallelization through Dask to complete the dataset in a few minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0553522779</td>\n",
       "      <td>799.0</td>\n",
       "      <td>July 28, 2015</td>\n",
       "      <td>4.78</td>\n",
       "      <td>1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1596471328</td>\n",
       "      <td>217944.0</td>\n",
       "      <td>January 1, 2007</td>\n",
       "      <td>4.63</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0981973353</td>\n",
       "      <td>711910.0</td>\n",
       "      <td>March 30, 2011</td>\n",
       "      <td>5.00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0822549328</td>\n",
       "      <td>8034749.0</td>\n",
       "      <td>August 1, 1998</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>061559722X</td>\n",
       "      <td>1069427.0</td>\n",
       "      <td>November 28, 2014</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1                  2     3     4\n",
       "0  0553522779      799.0      July 28, 2015  4.78  1122\n",
       "1  1596471328   217944.0    January 1, 2007  4.63     8\n",
       "2  0981973353   711910.0     March 30, 2011  5.00    12\n",
       "3  0822549328  8034749.0     August 1, 1998  4.00     1\n",
       "4  061559722X  1069427.0  November 28, 2014  5.00     4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(13535, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the IDs for the local HTML files\n",
    "htmlIDs = [re.findall(r'(.*).html', file)[0] for file in os.listdir(HTML_DIR) if file != '.DS_Store']\n",
    "\n",
    "dataset = pd.DataFrame(list(compute(*[delayed(extract_from_html)(ID) for ID in htmlIDs], scheduler='processes')))\n",
    "display(dataset.head(), dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename({0:'ID', 1:'Sales Rank', 2:'Date', 3:'Review Score', 4:'Review Count'}, axis=1)\n",
    "dataset.to_csv(COLLECTED_DATA_DIR + 'book_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:** Do the book IDs in the images folder, HTML folder and the dataset match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataIDs = dataset['ID']\n",
    "imageIDs = [re.findall(r'(.*).jpg', image)[0] for image in os.listdir(IMAGE_DIR) if image != '.DS_Store' and image != 'Icon\\r']\n",
    "htmlIDs = [re.findall(r'(.*).html', file)[0] for file in os.listdir(HTML_DIR) if file != '.DS_Store']\n",
    "\n",
    "set(dataIDs) == set(htmlIDs) == set(imageIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Collected Data\n",
    "---\n",
    "\n",
    "Now that we have the data collected, we should make sure it's clean before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sales Rank</th>\n",
       "      <th>Date</th>\n",
       "      <th>Review Score</th>\n",
       "      <th>Review Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0553522779</td>\n",
       "      <td>799.0</td>\n",
       "      <td>July 28, 2015</td>\n",
       "      <td>4.78</td>\n",
       "      <td>1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1596471328</td>\n",
       "      <td>217944.0</td>\n",
       "      <td>January 1, 2007</td>\n",
       "      <td>4.63</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0981973353</td>\n",
       "      <td>711910.0</td>\n",
       "      <td>March 30, 2011</td>\n",
       "      <td>5.00</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0822549328</td>\n",
       "      <td>8034749.0</td>\n",
       "      <td>August 1, 1998</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>061559722X</td>\n",
       "      <td>1069427.0</td>\n",
       "      <td>November 28, 2014</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  Sales Rank               Date  Review Score  Review Count\n",
       "0  0553522779       799.0      July 28, 2015          4.78          1122\n",
       "1  1596471328    217944.0    January 1, 2007          4.63             8\n",
       "2  0981973353    711910.0     March 30, 2011          5.00            12\n",
       "3  0822549328   8034749.0     August 1, 1998          4.00             1\n",
       "4  061559722X   1069427.0  November 28, 2014          5.00             4"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(COLLECTED_DATA_DIR + 'book_info.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many books we currently have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13535, 5)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which of the rows have missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sales Rank</th>\n",
       "      <th>Date</th>\n",
       "      <th>Review Score</th>\n",
       "      <th>Review Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1603800425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>B0056KOL7M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0789472570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>March 15, 2000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0750222247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>October 31, 1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>1910199427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  Sales Rank              Date  Review Score  Review Count\n",
       "61   1603800425         NaN               NaN           4.6            10\n",
       "86   B0056KOL7M         NaN               NaN           0.0             0\n",
       "206  0789472570         NaN    March 15, 2000           0.0             0\n",
       "363  0750222247         NaN  October 31, 1998           0.0             0\n",
       "384  1910199427         NaN               NaN           5.0             1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(142, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "missing = data[data['Sales Rank'].isna() | data['Date'].isna() | data['Review Score'].isna() | data['Review Count'].isna()]\n",
    "display(missing.head(), missing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at a bunch of these listings on Amazon, we see that the missing values are simply not given on their webpages. That is, they don't have a sales rank and/or their publishing date is not written. For this initial analysis, we drop these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID               object\n",
       "Sales Rank      float64\n",
       "Date             object\n",
       "Review Score    float64\n",
       "Review Count      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Date column could be converted into datetime, but we have to keep in mind that we don't have all the date information for each book. For some of them we have all of day, month and year; for some we only have month and year; for others we only have the year. So when we convert, we will see the first day of the month for the ones we don't have day data and we will see the first of January for the ones we don't have day or month data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'] = pd.to_datetime(data['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales Rank</th>\n",
       "      <th>Review Score</th>\n",
       "      <th>Review Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.339300e+04</td>\n",
       "      <td>13393.000000</td>\n",
       "      <td>13393.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.448022e+06</td>\n",
       "      <td>4.038758</td>\n",
       "      <td>126.155006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.821379e+06</td>\n",
       "      <td>1.500238</td>\n",
       "      <td>659.159407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.711300e+04</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.975380e+05</td>\n",
       "      <td>4.620000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.346889e+06</td>\n",
       "      <td>4.790000</td>\n",
       "      <td>73.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.147187e+07</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>27882.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sales Rank  Review Score  Review Count\n",
       "count  1.339300e+04  13393.000000  13393.000000\n",
       "mean   1.448022e+06      4.038758    126.155006\n",
       "std    2.821379e+06      1.500238    659.159407\n",
       "min    9.000000e+00      0.000000      0.000000\n",
       "25%    4.711300e+04      4.250000      4.000000\n",
       "50%    2.975380e+05      4.620000     21.000000\n",
       "75%    1.346889e+06      4.790000     73.000000\n",
       "max    2.147187e+07      5.000000  27882.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2015-09-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>1920-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>2018-06-12 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date\n",
       "count                 13393\n",
       "unique                 3285\n",
       "top     2015-09-01 00:00:00\n",
       "freq                     87\n",
       "first   1920-01-01 00:00:00\n",
       "last    2018-06-12 00:00:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Numerical features\n",
    "display(data.describe())\n",
    "\n",
    "# Date\n",
    "display(data.describe(include=np.datetime64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have books all the way from 1920 to a few months ago! We would also like to see if the sales ranks are unique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13214\n",
      "13393\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(data[\"Sales Rank\"].unique())}')\n",
    "print(f'{len(data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The duplicated sales ranks are interesting because we would expect each ranking to be unique; we should look into that. Let's look at the top one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sales Rank</th>\n",
       "      <th>Date</th>\n",
       "      <th>Review Score</th>\n",
       "      <th>Review Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8083</th>\n",
       "      <td>0843107588</td>\n",
       "      <td>777.0</td>\n",
       "      <td>2004-02-02</td>\n",
       "      <td>4.68</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9132</th>\n",
       "      <td>084313271X</td>\n",
       "      <td>777.0</td>\n",
       "      <td>2008-09-04</td>\n",
       "      <td>4.69</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12483</th>\n",
       "      <td>142630918X</td>\n",
       "      <td>777.0</td>\n",
       "      <td>2012-05-08</td>\n",
       "      <td>4.87</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID  Sales Rank       Date  Review Score  Review Count\n",
       "8083   0843107588       777.0 2004-02-02          4.68           277\n",
       "9132   084313271X       777.0 2008-09-04          4.69           247\n",
       "12483  142630918X       777.0 2012-05-08          4.87           175"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks, counts = np.unique(data['Sales Rank'], return_counts=True)\n",
    "\n",
    "duplicates = data[data['Sales Rank'] == ranks[np.argmax(counts)]]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to investigate is to scrape their ranks again from live data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8083     665\n",
       "9132     600\n",
       "12483    528\n",
       "dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates.apply(lambda row: extract_sales_rank(connect(row['ID'])[1]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sales ranks on Amazon can change very easily over time as millions of users buy items every second. We see that these books are quite close to the Sales Rank values they have in our data but all of them have changed slightly.\n",
    "\n",
    "Since the scraping is not done all at once and since the rankings keep changing, it is understandable that multiple books that have comparable sales have the same ranking at the time we scrape them. We decided that this is not a problem at this point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
