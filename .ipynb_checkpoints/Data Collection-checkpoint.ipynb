{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Data from Amazon\n",
    "---\n",
    "In this notebook, we scrape the missing parts of our dataset directly from Amazon using BeautifulSoup.\n",
    "\n",
    "The dataset we want:\n",
    "\n",
    "| ID | Review Score | Sales Rank | Category    | Title | Author | Date    | Visual Features     |\n",
    "| -- | ------------ | ---------- | ----------- | ----- | ------ | ------- | ------------------- |\n",
    "| |\n",
    "\n",
    "The dataset we have, as downloaded from [here](https://github.com/uchidalab/book-dataset):\n",
    "\n",
    "| ID | Filename | Image URL | Title | Author | Category ID | Category |\n",
    "| -- | -------- | --------- | ----- | ------ | ----------- | -------- |\n",
    "| |\n",
    "\n",
    "The `ID` column in the data can be used to access the webpage of each book, by connecting to https://www.amazon.com/dp/book-id. This allows us to scrape any data that is missing directly from Amazon.\n",
    "\n",
    "We already have the Title, Author and Category of each book ready to be used.\n",
    "\n",
    "For everything else, there's ~~Mastercard~~ BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To request data from Amazon\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# To open image links\n",
    "import urllib\n",
    "\n",
    "# To process data\n",
    "import pandas as pd\n",
    "\n",
    "# To extract information from weirdly formatted Amazon info\n",
    "import re\n",
    "\n",
    "# To create random delays\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "# To read data\n",
    "import csv\n",
    "\n",
    "# To check if a file is downloaded already\n",
    "import os\n",
    "\n",
    "# To print an image in the notebook programmatically\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Set data directories\n",
    "ORIGINAL_DATA_DIR = 'Original Data/'\n",
    "COLLECTED_DATA_DIR = 'Collected Data/'\n",
    "IMAGE_DIR = COLLECTED_DATA_DIR + 'Cover Images/'\n",
    "HTML_DIR = '/Users/dogatekin/Data/HTML Files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Image URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Category ID</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>761183272</td>\n",
       "      <td>0761183272.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61Y5cOdH...</td>\n",
       "      <td>Mom's Family Wall Calendar 2016</td>\n",
       "      <td>Sandra Boynton</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1623439671</td>\n",
       "      <td>1623439671.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61t-hrSw...</td>\n",
       "      <td>Doug the Pug 2016 Wall Calendar</td>\n",
       "      <td>Doug the Pug</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00O80WC6I</td>\n",
       "      <td>B00O80WC6I.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41X-KQqs...</td>\n",
       "      <td>Moleskine 2016 Weekly Notebook, 12M, Large, Bl...</td>\n",
       "      <td>Moleskine</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>761182187</td>\n",
       "      <td>0761182187.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61j-4gxJ...</td>\n",
       "      <td>365 Cats Color Page-A-Day Calendar 2016</td>\n",
       "      <td>Workman Publishing</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1578052084</td>\n",
       "      <td>1578052084.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51Ry4Tsq...</td>\n",
       "      <td>Sierra Club Engagement Calendar 2016</td>\n",
       "      <td>Sierra Club</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID        Filename  \\\n",
       "0   761183272  0761183272.jpg   \n",
       "1  1623439671  1623439671.jpg   \n",
       "2  B00O80WC6I  B00O80WC6I.jpg   \n",
       "3   761182187  0761182187.jpg   \n",
       "4  1578052084  1578052084.jpg   \n",
       "\n",
       "                                           Image URL  \\\n",
       "0  http://ecx.images-amazon.com/images/I/61Y5cOdH...   \n",
       "1  http://ecx.images-amazon.com/images/I/61t-hrSw...   \n",
       "2  http://ecx.images-amazon.com/images/I/41X-KQqs...   \n",
       "3  http://ecx.images-amazon.com/images/I/61j-4gxJ...   \n",
       "4  http://ecx.images-amazon.com/images/I/51Ry4Tsq...   \n",
       "\n",
       "                                               Title              Author  \\\n",
       "0                    Mom's Family Wall Calendar 2016      Sandra Boynton   \n",
       "1                    Doug the Pug 2016 Wall Calendar        Doug the Pug   \n",
       "2  Moleskine 2016 Weekly Notebook, 12M, Large, Bl...           Moleskine   \n",
       "3            365 Cats Color Page-A-Day Calendar 2016  Workman Publishing   \n",
       "4               Sierra Club Engagement Calendar 2016         Sierra Club   \n",
       "\n",
       "   Category ID   Category  \n",
       "0            3  Calendars  \n",
       "1            3  Calendars  \n",
       "2            3  Calendars  \n",
       "3            3  Calendars  \n",
       "4            3  Calendars  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_names = ['ID', 'Filename', 'Image URL', 'Title', 'Author', 'Category ID', 'Category']\n",
    "\n",
    "books = pd.read_csv(ORIGINAL_DATA_DIR + 'book32-listing.csv', encoding='latin1', header=None, names=header_names)\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calendars\n",
      "Comics & Graphic Novels\n",
      "Test Preparation\n",
      "Mystery, Thriller & Suspense\n",
      "Science Fiction & Fantasy\n",
      "Romance\n",
      "Humor & Entertainment\n",
      "Literature & Fiction\n",
      "Gay & Lesbian\n",
      "Engineering & Transportation\n",
      "Cookbooks, Food & Wine\n",
      "Crafts, Hobbies & Home\n",
      "Arts & Photography\n",
      "Education & Teaching\n",
      "Parenting & Relationships\n",
      "Self-Help\n",
      "Computers & Technology\n",
      "Medical Books\n",
      "Science & Math\n",
      "Health, Fitness & Dieting\n",
      "Business & Money\n",
      "Law\n",
      "Biographies & Memoirs\n",
      "History\n",
      "Politics & Social Sciences\n",
      "Reference\n",
      "Christian Books & Bibles\n",
      "Religion & Spirituality\n",
      "Sports & Outdoors\n",
      "Teen & Young Adult\n",
      "Children's Books\n",
      "Travel\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(books['Category'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want the Children's Books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Image URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>545790352</td>\n",
       "      <td>0545790352.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51MIi4p2...</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone: The Ill...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1419717014</td>\n",
       "      <td>1419717014.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61YgGsg-...</td>\n",
       "      <td>Diary of a Wimpy Kid: Old School</td>\n",
       "      <td>Jeff Kinney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1423160916</td>\n",
       "      <td>1423160916.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/611CmvkL...</td>\n",
       "      <td>Magnus Chase and the Gods of Asgard, Book 1: T...</td>\n",
       "      <td>Rick Riordan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1476789886</td>\n",
       "      <td>1476789886.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51KqU7Dw...</td>\n",
       "      <td>Rush Revere and the Star-Spangled Banner</td>\n",
       "      <td>Rush Limbaugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1338029991</td>\n",
       "      <td>1338029991.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61kvq74k...</td>\n",
       "      <td>Harry Potter Coloring Book</td>\n",
       "      <td>Scholastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID        Filename  \\\n",
       "0   545790352  0545790352.jpg   \n",
       "1  1419717014  1419717014.jpg   \n",
       "2  1423160916  1423160916.jpg   \n",
       "3  1476789886  1476789886.jpg   \n",
       "4  1338029991  1338029991.jpg   \n",
       "\n",
       "                                           Image URL  \\\n",
       "0  http://ecx.images-amazon.com/images/I/51MIi4p2...   \n",
       "1  http://ecx.images-amazon.com/images/I/61YgGsg-...   \n",
       "2  http://ecx.images-amazon.com/images/I/611CmvkL...   \n",
       "3  http://ecx.images-amazon.com/images/I/51KqU7Dw...   \n",
       "4  http://ecx.images-amazon.com/images/I/61kvq74k...   \n",
       "\n",
       "                                               Title         Author  \n",
       "0  Harry Potter and the Sorcerer's Stone: The Ill...   J.K. Rowling  \n",
       "1                   Diary of a Wimpy Kid: Old School    Jeff Kinney  \n",
       "2  Magnus Chase and the Gods of Asgard, Book 1: T...   Rick Riordan  \n",
       "3           Rush Revere and the Star-Spangled Banner  Rush Limbaugh  \n",
       "4                         Harry Potter Coloring Book     Scholastic  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books = books[books['Category'] == \"Children's Books\"].reset_index(drop=True)\n",
    "# We don't need the Category or Category ID columns anymore\n",
    "books.drop(columns=['Category ID', 'Category'], inplace=True)\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many books we have left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13605"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's fix the IDs in the dataset. For some reason, the ID column has the leading 0s removed (normally all of them should be 10 characters long), which makes the webpages inaccessible. The filename column has the correct IDs with the correct number of leading 0s. So let's use the Filename column as the new ID column, we can add the `.jpg` extension later when downloading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Image URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0545790352</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51MIi4p2...</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone: The Ill...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1419717014</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61YgGsg-...</td>\n",
       "      <td>Diary of a Wimpy Kid: Old School</td>\n",
       "      <td>Jeff Kinney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1423160916</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/611CmvkL...</td>\n",
       "      <td>Magnus Chase and the Gods of Asgard, Book 1: T...</td>\n",
       "      <td>Rick Riordan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1476789886</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51KqU7Dw...</td>\n",
       "      <td>Rush Revere and the Star-Spangled Banner</td>\n",
       "      <td>Rush Limbaugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1338029991</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61kvq74k...</td>\n",
       "      <td>Harry Potter Coloring Book</td>\n",
       "      <td>Scholastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                          Image URL  \\\n",
       "0  0545790352  http://ecx.images-amazon.com/images/I/51MIi4p2...   \n",
       "1  1419717014  http://ecx.images-amazon.com/images/I/61YgGsg-...   \n",
       "2  1423160916  http://ecx.images-amazon.com/images/I/611CmvkL...   \n",
       "3  1476789886  http://ecx.images-amazon.com/images/I/51KqU7Dw...   \n",
       "4  1338029991  http://ecx.images-amazon.com/images/I/61kvq74k...   \n",
       "\n",
       "                                               Title         Author  \n",
       "0  Harry Potter and the Sorcerer's Stone: The Ill...   J.K. Rowling  \n",
       "1                   Diary of a Wimpy Kid: Old School    Jeff Kinney  \n",
       "2  Magnus Chase and the Gods of Asgard, Book 1: T...   Rick Riordan  \n",
       "3           Rush Revere and the Star-Spangled Banner  Rush Limbaugh  \n",
       "4                         Harry Potter Coloring Book     Scholastic  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books['ID'] = books['Filename'].apply(lambda row: re.findall(u'(.*).jpg', row)[0])\n",
    "books.drop(columns='Filename', inplace=True)\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns we need to scrape are: `Review Score`, `Sales Rank` and `Date`. We also need to download the images from the URLs so that we can extract visual features from them, completing our dataset.\n",
    "\n",
    "First we will demonstrate the scraping process for each column on an arbitrary example, then we will combine these in a function and scrape the information for all the books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                                  0545790352\n",
       "Image URL    http://ecx.images-amazon.com/images/I/51MIi4p2...\n",
       "Title        Harry Potter and the Sorcerer's Stone: The Ill...\n",
       "Author                                            J.K. Rowling\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_book = books.iloc[0]\n",
    "example_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the webpage using the ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get('https://www.amazon.com/dp/' + example_book['ID'], headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36'})\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have access to the page content, we can turn it into a useful soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sales Rank and Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get both of these from the product details table on the webpage, which is in a table conveniently named `productDetailsTable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<b>Age Range:</b>,\n",
       " <b>Grade Level:</b>,\n",
       " <b>Series:</b>,\n",
       " <b>Hardcover:</b>,\n",
       " <b>Publisher:</b>,\n",
       " <b>Language:</b>,\n",
       " <b>ISBN-10:</b>,\n",
       " <b>ISBN-13:</b>,\n",
       " <b>\n",
       "     Product Dimensions: \n",
       "     </b>,\n",
       " <b>Shipping Weight:</b>,\n",
       " <b>Average Customer Review:</b>,\n",
       " <b>Amazon Best Sellers Rank:</b>,\n",
       " <b><a href=\"https://www.amazon.com/gp/bestsellers/books/3153/ref=pd_zg_hrsr_books_1_5_last/139-6933697-3555249\">Friendship</a></b>,\n",
       " <b><a href=\"https://www.amazon.com/gp/bestsellers/books/2967/ref=pd_zg_hrsr_books_2_3_last/139-6933697-3555249\">Action &amp; Adventure</a></b>,\n",
       " <b><a href=\"https://www.amazon.com/gp/bestsellers/books/3017/ref=pd_zg_hrsr_books_3_4_last/139-6933697-3555249\">Fantasy &amp; Magic</a></b>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('#productDetailsTable li b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use regex to extract the info we need from the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Rank: 115\n",
      "Date: October 6, 2015\n"
     ]
    }
   ],
   "source": [
    "for li in soup.select('#productDetailsTable li'):\n",
    "    # We only need two of the list items\n",
    "    if(li.b.string == 'Amazon Best Sellers Rank:'):\n",
    "        # The rank is given in the format #1,234,567\n",
    "        sales_rank = re.findall(u'#([\\d,]+)', li.b.nextSibling)[0]\n",
    "    elif(li.b.string == 'Publisher:'):\n",
    "        # The date is in the last set of parantheses\n",
    "        date = re.findall(u'\\(([^\\(\\)]*)\\)$', li.b.nextSibling)[0]\n",
    "        \n",
    "print(f'Sales Rank: {sales_rank}\\nDate: {date}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed there is also an item called `Average Customer Review` in the table we just used to extract the Rank and Date. Inside that item, all the review scores are found in a table with the id `histogramTable`, that gives the percentages of users for each score from 1 to 5 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 star87%4 star8%3 star2%2 star1%1 star2%'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = soup.select('#histogramTable')[0].text\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formatting is not great, but it's nothing we can't fix by using a simple regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5', '87'), ('4', '8'), ('3', '2'), ('2', '1'), ('1', '2')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = re.findall(u'(\\d) star(\\d+)%', reviews)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted average of these scores is our final Review Score for the given book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.77"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = 0\n",
    "for pair in reviews:\n",
    "    score += int(pair[0]) * int(pair[1])/100  # weights are percentages\n",
    "\n",
    "round(score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cover Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image URL of each book is available in the original dataset, let's make a HashMap of `ID:URL` pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://ecx.images-amazon.com/images/I/51MIi4p2YyL.jpg'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = books[['ID', 'Image URL']].set_index('ID').to_dict()['Image URL']\n",
    "example_url = urls[example_book['ID']]\n",
    "example_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![Example Image](http://ecx.images-amazon.com/images/I/51MIi4p2YyL.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(f'![Example Image]({example_url})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn it into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(book_id):\n",
    "    url = urls[book_id]\n",
    "    filename = book_id + '.jpg'\n",
    "    # Download only if not already downloaded\n",
    "    if not os.path.isfile(IMAGE_DIR + filename):\n",
    "        downloaded_img = urllib.request.urlopen(url)\n",
    "        f = open(IMAGE_DIR + filename, mode='wb')\n",
    "        f.write(downloaded_img.read())\n",
    "        downloaded_img.close()\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case we need some other information in the future from the webpages, let's save the raw HTML files somewhere so we don't have to scrape them from Amazon again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(book_id, html_text):\n",
    "    filename = book_id + '.html'\n",
    "    # Save only if not already saved\n",
    "    if not os.path.isfile(HTML_DIR + filename):\n",
    "        html_file = open(HTML_DIR + filename,\"w\")\n",
    "        html_file.write(html_text)\n",
    "        html_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bringing it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_info(book_id):\n",
    "    # Initial values\n",
    "    sales_rank = date = score = None\n",
    "    \n",
    "    # Trick the bot detector?\n",
    "    sleep(randint(1,3))\n",
    "    \n",
    "    # Get the soup of the relevant page\n",
    "    response = requests.get('https://www.amazon.com/dp/' + book_id, headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36'})\n",
    "    status = response.status_code\n",
    "    \n",
    "    if(status == 200):\n",
    "        # Successfully reached webpage\n",
    "        \n",
    "        # Save the raw HTML in case it is needed in the future\n",
    "        save_html(book_id, response.text)\n",
    "        \n",
    "        # Make some soup\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        # Get sales rank and date\n",
    "        for li in soup.select('#productDetailsTable li'):\n",
    "            if(li.b.string == 'Amazon Best Sellers Rank:'):\n",
    "                try:\n",
    "                    sales_rank = re.findall(u'#([\\d,]+)', li.b.nextSibling)[0]  # Format: #1,234,567\n",
    "                    sales_rank = int(sales_rank.replace(',',''))  # Remove the commas and convert to integer\n",
    "                except:\n",
    "                    sales_rank = None  # couldn't scrape\n",
    "            elif(li.b.string == 'Publisher:'):\n",
    "                try:\n",
    "                    date = re.findall(u'\\(([^\\(\\)]*)\\)$', li.b.nextSibling)[0]  # Format: Inside last parantheses\n",
    "                except:\n",
    "                    date = None  # couldn't scrape\n",
    "\n",
    "        # Get average review score\n",
    "        try:\n",
    "            reviews = soup.select('#histogramTable')[0].text\n",
    "            reviews = re.findall(u'(\\d) star(\\d+)%', reviews)\n",
    "\n",
    "            score = 0\n",
    "            for pair in reviews:\n",
    "                score += int(pair[0]) * int(pair[1])/100  # weights are percentages\n",
    "                \n",
    "            score = round(score, 3)\n",
    "        except:\n",
    "            score = None  # couldn't scrape\n",
    "            \n",
    "        # Download the cover image\n",
    "        download_image(book_id)\n",
    "    else:\n",
    "        # Could not reach webpage\n",
    "        sales_rank = date = score = f'Error {status}'\n",
    "\n",
    "    return status, book_id, sales_rank, date, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a final test on the example book we used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, '0545790352', 115, 'October 6, 2015', 4.77)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped = scrape_info(example_book['ID'])\n",
    "scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to stop and continue at will, we will write the scraped info to a csv file as we go along, and simultaneously download cover images. Let's initialize this file with a meaningful header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(COLLECTED_DATA_DIR + 'scraped.csv', 'a') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['ID', 'Sales Rank', 'Date', 'Review Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go through the dataset, starting scraping from where we last left off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 1593693400 | Scrape Info: Success\n",
      "ID: 1933241144 | Scrape Info: Success\n",
      "ID: 1933241241 | Scrape Info: Success\n",
      "ID: 0545561663 | Scrape Info: Success\n",
      "ID: 1442487712 | Scrape Info: Success\n",
      "ID: 1591744490 | Scrape Info: Success\n",
      "ID: 076110478X | Scrape Info: Success\n",
      "ID: 193324111X | Scrape Info: Success\n",
      "ID: 1935800434 | Scrape Info: Success\n",
      "ID: 0761313427 | Scrape Info: Success\n",
      "ID: 1633220508 | Scrape Info: Success\n",
      "ID: 1933241136 | Scrape Info: Success\n",
      "ID: 160958970X | Scrape Info: Success\n",
      "ID: 0764145797 | Scrape Info: Success\n",
      "ID: 0802774407 | Scrape Info: Success\n",
      "ID: 1250031109 | Scrape Info: Success\n",
      "ID: 1442435712 | Scrape Info: Success\n",
      "ID: 0764166115 | Scrape Info: Success\n",
      "ID: 1465422994 | Scrape Info: Success\n",
      "ID: 0756697875 | Scrape Info: Success\n",
      "ID: 1935800205 | Scrape Info: Success\n",
      "ID: 0545459907 | Scrape Info: Success\n",
      "ID: 1584859776 | Scrape Info: Success\n",
      "ID: 1933241128 | Scrape Info: Success\n",
      "ID: 0694006386 | Scrape Info: Success\n",
      "ID: 1440327068 | Scrape Info: Success\n",
      "ID: 0486250571 | Scrape Info: Success\n",
      "ID: 0486433269 | Scrape Info: Success\n",
      "ID: 1465401725 | Scrape Info: Success\n",
      "ID: 1604381833 | Scrape Info: Success\n",
      "ID: 1892951452 | Scrape Info: Success\n",
      "ID: 0486460673 | Scrape Info: Success\n",
      "ID: 0375823751 | Scrape Info: Success\n",
      "ID: 159174802X | Scrape Info: Success\n",
      "ID: 076116927X | Scrape Info: Success\n",
      "ID: 1584766247 | Scrape Info: Success\n",
      "ID: 0873588770 | Scrape Info: Success\n",
      "ID: 1507745923 | Scrape Info: Success\n",
      "ID: 1940787092 | Scrape Info: Success\n",
      "ID: 1426319193 | Scrape Info: Success\n",
      "ID: 0448487152 | Scrape Info: Success\n",
      "ID: 0545823269 | Scrape Info: Success\n",
      "ID: 1465419217 | Scrape Info: Success\n",
      "ID: 0486447103 | Scrape Info: Success\n",
      "ID: 0545823234 | Scrape Info: Success\n",
      "ID: 0761166629 | Scrape Info: Success\n",
      "ID: 0761184570 | Scrape Info: Success\n",
      "ID: 1250047757 | Scrape Info: Success\n",
      "ID: 1575423596 | Scrape Info: Success\n",
      "ID: 1580626874 | Scrape Info: Success\n",
      "ID: 0843107588 | Scrape Info: Success\n",
      "ID: 1605531278 | Scrape Info: Success\n",
      "ID: 0761166513 | Scrape Info: Success\n",
      "ID: 0887432735 | Scrape Info: Success\n",
      "ID: 0545823242 | Scrape Info: Success\n",
      "ID: 1465435905 | Scrape Info: Success\n",
      "ID: 0545415837 | Scrape Info: Success\n",
      "ID: 0887432417 | Scrape Info: Success\n",
      "ID: 1580175384 | Scrape Info: Success\n",
      "ID: 0486418634 | Scrape Info: Success\n",
      "ID: 1426309333 | Scrape Info: Success\n",
      "ID: 0761166599 | Scrape Info: Success\n",
      "ID: 0486468933 | Scrape Info: Success\n",
      "ID: 0761166602 | Scrape Info: Success\n",
      "ID: 0486465810 | Scrape Info: Success\n",
      "ID: 0986444162 | Scrape Info: Success\n",
      "ID: 0761166637 | Scrape Info: Success\n",
      "ID: 0439708702 | Scrape Info: Success\n",
      "ID: 0843115890 | Scrape Info: Success\n",
      "ID: 048625156X | Scrape Info: Success\n",
      "ID: 054568515X | Scrape Info: Success\n",
      "ID: 0689038623 | Scrape Info: Success\n",
      "ID: 0545685168 | Scrape Info: Success\n",
      "ID: 1452108196 | Scrape Info: Success\n",
      "ID: 0789454726 | Scrape Info: Success\n",
      "ID: 0843120363 | Scrape Info: Success\n",
      "ID: 0486297810 | Scrape Info: Success\n",
      "ID: 0843110333 | Scrape Info: Success\n",
      "ID: 0761166521 | Scrape Info: Success\n",
      "ID: 0544339207 | Scrape Info: Success\n",
      "ID: 0688088090 | Scrape Info: Success\n",
      "ID: 0805098518 | Scrape Info: Success\n",
      "ID: 0761167560 | Scrape Info: Success\n",
      "ID: 0545415845 | Scrape Info: Success\n",
      "ID: 0439042445 | Scrape Info: Success\n",
      "ID: 1580625584 | Scrape Info: Success\n",
      "ID: 0316736090 | Scrape Info: Success\n",
      "ID: 0843119217 | Scrape Info: Success\n",
      "ID: 1426320701 | Scrape Info: Success\n",
      "ID: 1426308647 | Scrape Info: Success\n",
      "ID: 142631194X | Scrape Info: Success\n",
      "ID: 076116653X | Scrape Info: Success\n",
      "ID: 0843128275 | Scrape Info: Success\n",
      "ID: 0486413373 | Scrape Info: Success\n",
      "ID: 0486270106 | Scrape Info: Success\n",
      "ID: 0938256815 | Scrape Info: Success\n",
      "ID: 075661807X | Scrape Info: Success\n",
      "ID: 0439763096 | Scrape Info: Success\n",
      "ID: 0061992275 | Scrape Info: Success\n",
      "ID: 1426308469 | Scrape Info: Success\n",
      "ID: 0060245867 | Scrape Info: Success\n",
      "ID: 0374360979 | Scrape Info: Success\n",
      "ID: 0374300216 | Scrape Info: Success\n",
      "ID: 0439738199 | Scrape Info: Success\n",
      "ID: 0316380865 | Scrape Info: Success\n",
      "ID: 0679890475 | Scrape Info: Success\n",
      "ID: 0671493183 | Scrape Info: Success\n",
      "ID: 0062304240 | Scrape Info: Success\n",
      "ID: 142319957X | Scrape Info: Success\n",
      "ID: 0394800133 | Scrape Info: Success\n",
      "ID: 0312510799 | Scrape Info: Success\n",
      "ID: 0553523910 | Scrape Info: Success\n",
      "ID: 044845694X | Scrape Info: Success\n",
      "ID: 067144901X | Scrape Info: Success\n",
      "ID: 0061906220 | Scrape Info: Success\n",
      "ID: 067001396X | Scrape Info: Success\n",
      "ID: 0307157857 | Scrape Info: Success\n",
      "ID: 0679800913 | Scrape Info: Success\n",
      "ID: 0545153530 | Scrape Info: Success\n",
      "ID: 0062304275 | Scrape Info: Success\n",
      "ID: 0062304305 | Scrape Info: Success\n",
      "ID: 0316070025 | Scrape Info: Success\n",
      "ID: 039480001X | Scrape Info: Success\n",
      "ID: 0307021343 | Scrape Info: Success\n",
      "ID: 1484722620 | Scrape Info: Success\n",
      "ID: 0670015792 | Scrape Info: Success\n",
      "ID: 0545855721 | Scrape Info: Success\n",
      "ID: 0062198696 | Scrape Info: Success\n",
      "ID: 0140501738 | Scrape Info: Success\n",
      "ID: 0679809015 | Scrape Info: Success\n",
      "ID: 054584231X | Scrape Info: Success\n",
      "ID: 078681988X | Scrape Info: Success\n",
      "ID: 0545685370 | Scrape Info: Success\n",
      "ID: 0399230033 | Scrape Info: Success\n",
      "ID: 0736430806 | Scrape Info: Success\n",
      "ID: 068982663X | Scrape Info: Success\n",
      "ID: 1492616516 | Scrape Info: Success\n",
      "ID: 1484722841 | Scrape Info: Success\n",
      "ID: 0312511078 | Scrape Info: Success\n",
      "ID: 0823413209 | Scrape Info: Success\n",
      "ID: 1423179587 | Scrape Info: Success\n",
      "ID: 1512183687 | Scrape Info: Success\n",
      "ID: 0756689899 | Scrape Info: Success\n",
      "ID: 0062110586 | Scrape Info: Success\n",
      "ID: 0590417010 | Scrape Info: Success\n",
      "ID: 0307155102 | Scrape Info: Success\n",
      "ID: 0440412676 | Scrape Info: Success\n",
      "ID: 0394826817 | Scrape Info: Success\n",
      "ID: 0312513062 | Scrape Info: Success\n",
      "ID: 1423190874 | Scrape Info: Success\n",
      "ID: 0375849912 | Scrape Info: Success\n",
      "ID: 1933718005 | Scrape Info: Success\n",
      "ID: 0763657093 | Scrape Info: Success\n",
      "ID: 0670059838 | Scrape Info: Success\n",
      "ID: 0545834910 | Scrape Info: Success\n",
      "ID: 0763644323 | Scrape Info: Success\n",
      "ID: 0694016381 | Scrape Info: Success\n",
      "ID: 0394800206 | Scrape Info: Success\n",
      "ID: 0399240462 | Scrape Info: Success\n",
      "ID: 0547391005 | Scrape Info: Success\n",
      "ID: 0064441865 | Scrape Info: Success\n",
      "ID: 0547516185 | Scrape Info: Success\n",
      "ID: 0803703740 | Scrape Info: Success\n",
      "ID: 0307930084 | Scrape Info: Success\n",
      "ID: 1442435828 | Scrape Info: Success\n",
      "ID: 1423171012 | Scrape Info: Success\n",
      "ID: 054511506X | Scrape Info: Success\n",
      "ID: 0544083539 | Scrape Info: Success\n",
      "ID: 0142414530 | Scrape Info: Success\n",
      "ID: 0395137209 | Scrape Info: Success\n",
      "ID: 0763661635 | Scrape Info: Success\n",
      "ID: 1442430540 | Scrape Info: Success\n",
      "ID: 0679893857 | Scrape Info: Success\n",
      "ID: 0152163565 | Scrape Info: Success\n",
      "ID: 1846860733 | Scrape Info: Success\n",
      "ID: 006200400X | Scrape Info: Success\n",
      "ID: 0140365443 | Scrape Info: Success\n",
      "ID: 0152058508 | Scrape Info: Success\n",
      "ID: 1936140136 | Scrape Info: Success\n",
      "ID: 0395181577 | Scrape Info: Success\n",
      "ID: 0989341453 | Scrape Info: Success\n",
      "ID: 080757824X | Scrape Info: Success\n",
      "ID: 006440434X | Scrape Info: Success\n",
      "ID: 1442430486 | Scrape Info: Success\n",
      "ID: 0545506719 | Scrape Info: Success\n",
      "ID: 0747541930 | Scrape Info: Success\n",
      "ID: 0064442489 | Scrape Info: Success\n",
      "ID: 0140365419 | Scrape Info: Success\n",
      "ID: 0140368434 | Scrape Info: Success\n",
      "ID: 0544339185 | Scrape Info: Success\n",
      "ID: 0316563447 | Scrape Info: Success\n",
      "ID: 1442430508 | Scrape Info: Success\n",
      "ID: 0794419089 | Scrape Info: Success\n",
      "ID: 014037602X | Scrape Info: Success\n",
      "ID: 0553508423 | Scrape Info: Success\n",
      "ID: 0544302362 | Scrape Info: Success\n",
      "ID: 0064430782 | Scrape Info: Success\n",
      "ID: 1596438568 | Scrape Info: Success\n",
      "ID: 055350844X | Scrape Info: Success\n",
      "ID: 0399257632 | Scrape Info: Success\n",
      "ID: 0374380503 | Scrape Info: Success\n",
      "ID: 0060526289 | Scrape Info: Success\n",
      "ID: 0061921157 | Scrape Info: Success\n",
      "ID: 055351234X | Scrape Info: Success\n",
      "ID: 0698118847 | Scrape Info: Success\n",
      "ID: 1619631695 | Scrape Info: Success\n",
      "ID: 0761389377 | Scrape Info: Success\n",
      "ID: 0060777540 | Scrape Info: Success\n",
      "ID: 0140368426 | Scrape Info: Success\n",
      "ID: 0689839928 | Scrape Info: Success\n",
      "ID: 0140377085 | Scrape Info: Success\n",
      "ID: 0763643270 | Scrape Info: Success\n",
      "ID: 1423174372 | Scrape Info: Success\n",
      "ID: 0735841632 | Scrape Info: Success\n",
      "ID: 0061473723 | Scrape Info: Success\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0140380329 | Scrape Info: Success\n",
      "ID: 0395253780 | Scrape Info: Success\n",
      "ID: 1582462062 | Scrape Info: Success\n",
      "ID: 0395273986 | Scrape Info: Success\n",
      "ID: 0544236092 | Scrape Info: Success\n",
      "ID: 0553512374 | Scrape Info: Success\n",
      "ID: 0689840810 | Scrape Info: Success\n",
      "ID: 0395174511 | Scrape Info: Success\n",
      "ID: 0340486988 | Scrape Info: Success\n",
      "ID: 0689821883 | Scrape Info: Success\n",
      "ID: 1929132670 | Scrape Info: Success\n",
      "ID: 1589251520 | Scrape Info: Success\n",
      "ID: 0062345192 | Scrape Info: Success\n",
      "ID: 1845074920 | Scrape Info: Success\n",
      "ID: 0375847510 | Scrape Info: Success\n",
      "ID: 0547223234 | Scrape Info: Success\n",
      "ID: 8494171674 | Scrape Info: Success\n",
      "ID: 0812063430 | Scrape Info: Success\n",
      "ID: 0152048472 | Scrape Info: Success\n",
      "ID: B005SN42LM | Scrape Info: Success\n",
      "ID: 0448420953 | Scrape Info: Success\n",
      "ID: 0140547347 | Scrape Info: Success\n",
      "ID: 1454901306 | Scrape Info: Success\n",
      "ID: 1607107031 | Scrape Info: Success\n",
      "ID: 015205961X | Scrape Info: Success\n",
      "ID: 0316283355 | Scrape Info: Success\n",
      "ID: 015204955X | Scrape Info: Success\n",
      "ID: 1933212322 | Scrape Info: Success\n",
      "ID: 1620867389 | Scrape Info: Success\n",
      "ID: 0060088591 | Scrape Info: Success\n",
      "ID: 1419701266 | Scrape Info: Success\n",
      "ID: 0375851313 | Scrape Info: Success\n",
      "ID: B000OVLNN8 | Scrape Info: Success\n",
      "ID: 1846436036 | Scrape Info: Success\n",
      "ID: 1941437567 | Scrape Info: Success\n",
      "ID: 1589800680 | Scrape Info: Success\n",
      "ID: 156554286X | Scrape Info: Success\n",
      "ID: 1582462399 | Scrape Info: Success\n",
      "ID: 1586853139 | Scrape Info: Success\n",
      "ID: 156554773X | Scrape Info: Success\n",
      "ID: 0698118979 | Scrape Info: Success\n",
      "ID: 0374380643 | Scrape Info: Success\n",
      "ID: 0007586752 | Scrape Info: Success\n",
      "ID: 0547896913 | Scrape Info: Success\n",
      "ID: 0064431436 | Scrape Info: Success\n",
      "ID: 0395978351 | Scrape Info: Success\n",
      "ID: 0544320794 | Scrape Info: Success\n",
      "ID: 0679890483 | Scrape Info: Success\n",
      "ID: 0618800425 | Scrape Info: Success\n",
      "ID: 0618494960 | Scrape Info: Success\n",
      "ID: 0547487045 | Scrape Info: Success\n",
      "ID: 0316045462 | Scrape Info: Success\n",
      "ID: 0618120718 | Scrape Info: Success\n",
      "ID: 0547968183 | Scrape Info: Success\n",
      "ID: 054729963X | Scrape Info: Success\n",
      "ID: 0547914164 | Scrape Info: Success\n",
      "ID: 0618538224 | Scrape Info: Success\n",
      "ID: 061891952X | Scrape Info: Success\n",
      "ID: 0544252306 | Scrape Info: Success\n",
      "ID: 0618154248 | Scrape Info: Success\n",
      "ID: 0861716833 | Scrape Info: Success\n",
      "ID: 0547131062 | Scrape Info: Success\n",
      "ID: 0375867740 | Scrape Info: Success\n",
      "ID: 0763615765 | Scrape Info: Success\n",
      "ID: 0547523106 | Scrape Info: Success\n",
      "ID: 0618663738 | Scrape Info: Success\n",
      "ID: 0547577222 | Scrape Info: Success\n",
      "ID: 0618065695 | Scrape Info: Success\n",
      "ID: 0618605649 | Scrape Info: Success\n",
      "ID: 054443062X | Scrape Info: Success\n",
      "ID: 054775731X | Scrape Info: Success\n",
      "ID: 039591907X | Scrape Info: Success\n",
      "ID: 0618065679 | Scrape Info: Success\n",
      "ID: 0547818521 | Scrape Info: Success\n",
      "ID: 0440415802 | Scrape Info: Success\n",
      "ID: 0671562711 | Scrape Info: Success\n",
      "ID: 039515023X | Scrape Info: Success\n",
      "ID: 0547875274 | Scrape Info: Success\n",
      "ID: 054438072X | Scrape Info: Success\n",
      "ID: 0547396317 | Scrape Info: Success\n",
      "ID: 0395070627 | Scrape Info: Success\n",
      "ID: 0547516894 | Scrape Info: Success\n",
      "ID: 0547238630 | Scrape Info: Success\n",
      "ID: 0547238738 | Scrape Info: Success\n",
      "ID: 1479522864 | Scrape Info: Success\n",
      "ID: 1452112509 | Scrape Info: Success\n",
      "ID: 0618462643 | Scrape Info: Success\n",
      "ID: 0544107934 | Scrape Info: Success\n",
      "ID: 0395919088 | Scrape Info: Success\n",
      "ID: 142631518X | Scrape Info: Success\n",
      "ID: 0395919096 | Scrape Info: Success\n",
      "ID: 0544109880 | Scrape Info: Success\n",
      "ID: 0618663754 | Scrape Info: Success\n",
      "ID: 006443401X | Scrape Info: Success\n",
      "ID: 0544652266 | Scrape Info: Success\n",
      "ID: 1423160657 | Scrape Info: Success\n",
      "ID: 0547595298 | Scrape Info: Success\n",
      "ID: 0545165776 | Scrape Info: Success\n",
      "ID: 0547243006 | Scrape Info: Success\n",
      "ID: 0544146875 | Scrape Info: Success\n",
      "ID: 0618891994 | Scrape Info: Success\n",
      "ID: 1442450711 | Scrape Info: Success\n",
      "ID: 006085281X | Scrape Info: Success\n",
      "ID: 0547138415 | Scrape Info: Success\n",
      "ID: 0736928324 | Scrape Info: Success\n",
      "ID: 0395912148 | Scrape Info: Success\n",
      "ID: 0618999868 | Scrape Info: Success\n",
      "ID: 1426311060 | Scrape Info: Success\n",
      "ID: 0811877175 | Scrape Info: Success\n",
      "ID: 0618226117 | Scrape Info: Success\n",
      "ID: 054750425X | Scrape Info: Success\n",
      "ID: 0618051589 | Scrape Info: Success\n",
      "ID: 0618800417 | Scrape Info: Success\n",
      "ID: 0547337876 | Scrape Info: Success\n",
      "ID: 0618956700 | Scrape Info: Success\n",
      "ID: 0618605878 | Scrape Info: Success\n",
      "ID: 0545559766 | Scrape Info: Success\n",
      "ID: 0544110005 | Scrape Info: Success\n",
      "ID: 0618891986 | Scrape Info: Success\n",
      "ID: 054751705X | Scrape Info: Success\n",
      "ID: 0395899257 | Scrape Info: Success\n",
      "ID: 0618663770 | Scrape Info: Success\n",
      "ID: 0545559804 | Scrape Info: Success\n",
      "ID: 054425001X | Scrape Info: Success\n",
      "ID: 0679872841 | Scrape Info: Success\n",
      "ID: 1477847170 | Scrape Info: Success\n",
      "ID: 0545559847 | Scrape Info: Success\n",
      "ID: 054774451X | Scrape Info: Success\n",
      "ID: 0544228014 | Scrape Info: Success\n",
      "ID: 0618603875 | Scrape Info: Success\n",
      "ID: 0395912156 | Scrape Info: Success\n",
      "ID: 0881063134 | Scrape Info: Success\n",
      "ID: 0761158596 | Scrape Info: Success\n",
      "ID: 1452112517 | Scrape Info: Success\n",
      "ID: 0811870545 | Scrape Info: Success\n",
      "ID: 1442445572 | Scrape Info: Success\n",
      "ID: 0811867870 | Scrape Info: Success\n",
      "ID: 0811857719 | Scrape Info: Success\n",
      "ID: 0486433315 | Scrape Info: Success\n",
      "ID: 145212650X | Scrape Info: Success\n",
      "ID: 0761174834 | Scrape Info: Success\n",
      "ID: 0811848485 | Scrape Info: Success\n",
      "ID: 0899197965 | Scrape Info: Success\n",
      "ID: 0811873447 | Scrape Info: Success\n",
      "ID: 0312498586 | Scrape Info: Success\n",
      "ID: 0062335367 | Scrape Info: Success\n",
      "ID: 0811875148 | Scrape Info: Success\n",
      "ID: 0718030516 | Scrape Info: Success\n",
      "ID: 1452140286 | Scrape Info: Success\n",
      "ID: 0545661668 | Scrape Info: Success\n",
      "ID: 081187513X | Scrape Info: Success\n",
      "ID: 0545509165 | Scrape Info: Success\n",
      "ID: 0811873404 | Scrape Info: Success\n",
      "ID: 1628552174 | Scrape Info: Success\n",
      "ID: 1941069347 | Scrape Info: Success\n",
      "ID: 081186023X | Scrape Info: Success\n",
      "ID: 081186104X | Scrape Info: Success\n",
      "ID: 0688148999 | Scrape Info: Success\n",
      "ID: 055351167X | Scrape Info: Success\n",
      "ID: 1934649457 | Scrape Info: Success\n",
      "ID: 1426314825 | Scrape Info: Success\n",
      "ID: 0544531213 | Scrape Info: Success\n",
      "ID: 0399167420 | Scrape Info: Success\n",
      "ID: 0811852369 | Scrape Info: Success\n",
      "ID: 1596439254 | Scrape Info: Success\n",
      "ID: 0811848477 | Scrape Info: Success\n",
      "ID: 0547959095 | Scrape Info: Success\n",
      "ID: 0811852350 | Scrape Info: Success\n",
      "ID: 145210221X | Scrape Info: Success\n",
      "ID: 1452134316 | Scrape Info: Success\n",
      "ID: 1595723625 | Scrape Info: Success\n",
      "ID: 0486258343 | Scrape Info: Success\n",
      "ID: 0547367554 | Scrape Info: Success\n",
      "ID: 055351170X | Scrape Info: Success\n",
      "ID: 0811866556 | Scrape Info: Success\n",
      "ID: 0811857700 | Scrape Info: Success\n",
      "ID: 1499212941 | Scrape Info: Success\n",
      "ID: 1627791434 | Scrape Info: Success\n",
      "ID: 0811854574 | Scrape Info: Success\n",
      "ID: 1590172264 | Scrape Info: Success\n",
      "ID: 1609924665 | Scrape Info: Success\n",
      "ID: 1627791426 | Scrape Info: Success\n",
      "ID: 1627791418 | Scrape Info: Success\n",
      "ID: 0761179801 | Scrape Info: Success\n",
      "ID: 0823085732 | Scrape Info: Success\n",
      "ID: 0062342290 | Scrape Info: Success\n",
      "ID: 1627791442 | Scrape Info: Success\n",
      "ID: 1452134308 | Scrape Info: Success\n",
      "ID: 1452106428 | Scrape Info: Success\n",
      "ID: 0689806876 | Scrape Info: Success\n",
      "ID: 1452112525 | Scrape Info: Success\n",
      "ID: 0761179798 | Scrape Info: Success\n",
      "ID: 1452106436 | Scrape Info: Success\n",
      "ID: 1442408340 | Scrape Info: Success\n",
      "ID: 0439448611 | Scrape Info: Success\n",
      "ID: 0545852285 | Scrape Info: Success\n",
      "ID: 0764165127 | Scrape Info: Success\n",
      "ID: 0763670693 | Scrape Info: Success\n",
      "ID: 0544149440 | Scrape Info: Success\n",
      "ID: 0811867889 | Scrape Info: Success\n",
      "ID: 1426315120 | Scrape Info: Success\n",
      "ID: 1623540283 | Scrape Info: Success\n",
      "ID: 162832113X | Scrape Info: Success\n",
      "ID: 1452108110 | Scrape Info: Success\n",
      "ID: 0811863557 | Scrape Info: Success\n",
      "ID: 1595723609 | Scrape Info: Success\n",
      "ID: 0553507796 | Scrape Info: Success\n",
      "ID: 1442422661 | Scrape Info: Success\n",
      "ID: 1609052803 | Scrape Info: Success\n",
      "ID: 1426315082 | Scrape Info: Success\n",
      "ID: 0811869741 | Scrape Info: Success\n",
      "ID: 0449814343 | Scrape Info: Success\n",
      "ID: 0448487683 | Scrape Info: Success\n",
      "ID: 0807556009 | Scrape Info: Success\n",
      "ID: 192748393X | Scrape Info: Success\n",
      "ID: 054773851X | Scrape Info: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-2f3079bb4dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'ID: {current_id} | Scrape Info: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mscraped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscraped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m503\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-1c1817755a9f>\u001b[0m in \u001b[0;36mscrape_info\u001b[0;34m(book_id)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Download the cover image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mdownload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Could not reach webpage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-100-bd09496f75ff>\u001b[0m in \u001b[0;36mdownload_image\u001b[0;34m(book_id)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdownloaded_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownloaded_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mdownloaded_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(COLLECTED_DATA_DIR + 'scraped.csv', 'a+') as file:\n",
    "    reader = csv.reader(file)\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Look at the last scraped book to continue from the next one in the dataset\n",
    "    file.seek(0)\n",
    "    last_scraped = next(reversed(list(reader)))[0]\n",
    "    \n",
    "    if(last_scraped == 'ID'):\n",
    "        # Nothing was scraped yet, start from the beginning\n",
    "        index = 0\n",
    "    else:\n",
    "        # At least one book was scraped, find the index of the last scraped book and start from the next one\n",
    "        last_scraped_index = books.index[books['ID'] == last_scraped].tolist()[0]\n",
    "        index = last_scraped_index + 1\n",
    "        \n",
    "    blocked = False\n",
    "    while not blocked:\n",
    "        current_id = books.iloc[index]['ID']\n",
    "        print(f'ID: {current_id} | Scrape Info: ', end='')\n",
    "\n",
    "        scraped = scrape_info(current_id)\n",
    "        \n",
    "        if(scraped[0] == 503):\n",
    "            blocked = True\n",
    "            break\n",
    "        \n",
    "        writer.writerow(scraped[1:])\n",
    "        file.flush()\n",
    "        print(f'Success')\n",
    "            \n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shady Shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torrequest import TorRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tor = TorRequest(password='yoyoyo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Original IP Address: 185.25.193.110\n"
     ]
    }
   ],
   "source": [
    "response= requests.get('http://ipecho.net/plain')\n",
    "print (\"My Original IP Address:\",response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Ip Address 192.42.116.23\n"
     ]
    }
   ],
   "source": [
    "tor.reset_identity() #Reset Tor\n",
    "response = tor.get('http://ipecho.net/plain')\n",
    "print (\"New Ip Address\",response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tor.reset_identity()\n",
    "response = tor.get('https://www.amazon.com/dp/' + example_book['ID'], headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36'})\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \n",
    "html_file = open(HTML_DIR + \"someid.html\",\"w\")\n",
    "html_file.write(request.text)\n",
    "html_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
