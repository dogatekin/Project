{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Data from Amazon\n",
    "---\n",
    "In this notebook, we scrape the missing parts of our dataset directly from Amazon using BeautifulSoup.\n",
    "\n",
    "The dataset we want:\n",
    "\n",
    "| Book ID | Review Score | Sales Rank | Category    | Title | Author | Year    | Visual Features     |\n",
    "| ------- | ------------ | ---------- | ----------- | ----- | ------ | ------- | ------------------- |\n",
    "| |\n",
    "\n",
    "<!---| Numeric | Numeric    | Numeric    | Categorical | Numeric | Numeric | Numeric/Categorical |--->\n",
    "\n",
    "The data we have, as downloaded from [here](http://jmcauley.ucsd.edu/data/amazon/):\n",
    "\n",
    "| asin | helpful | overall | reviewText | reviewTime | reviewerID | reviewerName | summary | unixReviewTime |\n",
    "| ---- | ------- | ------- | ---------- | ---------- | ---------- | ------------ | ------- | -------------- |\n",
    "| |\n",
    "\n",
    "The `asin` column in the data we have corresponds to Book IDs, so we have that column already and more importantly, we have a way to reach the webpage of every book in the data—by accessing https://www.amazon.com/dp/book-id-here.\n",
    "\n",
    "We can also get the review score of each book without scraping it, by taking the average of all the reviews each book received.\n",
    "\n",
    "For everything else, there's ~~Mastercard~~ BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "DATA_DIR = '/Users/dogatekin/Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have the review data in Parquet format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(asin='000100039X', helpful=[0, 0], overall=5.0, reviewText='Spiritually and mentally inspiring! A book that allows you to question your morals and will help you discover who you really are!', reviewTime='12 16, 2012', reviewerID='A10000012B7CGYKOMPQ4L', reviewerName='Adam', summary='Wonderful!', unixReviewTime=1355616000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = spark.read.parquet(DATA_DIR + \"reviews.parquet\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many unique books we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT asin)|\n",
      "+--------------------+\n",
      "|              367982|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.select(countDistinct('asin')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not too bad! We might as well do the aggregation for the average review score of each book and move over to Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------------+\n",
      "|      asin|      avg(overall)|count(overall)|\n",
      "+----------+------------------+--------------+\n",
      "|0002216973| 4.833333333333333|            12|\n",
      "|0006476155| 4.299418604651163|           344|\n",
      "|0006544150| 4.222222222222222|             9|\n",
      "|0006550479|3.5428571428571427|            35|\n",
      "|0007163932|               4.5|            22|\n",
      "|0023605103|               4.2|             5|\n",
      "|0027861317|             4.625|             8|\n",
      "|0028633784| 4.428571428571429|             7|\n",
      "|0060087447| 3.606060606060606|            33|\n",
      "|0060192097| 4.555555555555555|            18|\n",
      "|0060392436| 4.857142857142857|             7|\n",
      "|0060532564| 4.571428571428571|            21|\n",
      "|0060540745| 4.228571428571429|            35|\n",
      "|0060574437| 4.857142857142857|             7|\n",
      "|0060598808|3.8271604938271606|            81|\n",
      "|0060611561|              4.35|            20|\n",
      "|0060731451|4.2631578947368425|            19|\n",
      "|0060753080|               2.0|            19|\n",
      "|0060753684| 4.888888888888889|             9|\n",
      "|0060763035| 4.266666666666667|            45|\n",
      "+----------+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books = reviews.groupBy('asin').agg(avg('overall'), count('overall')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>avg(overall)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002216973</td>\n",
       "      <td>4.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0006476155</td>\n",
       "      <td>4.299419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0006544150</td>\n",
       "      <td>4.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0006550479</td>\n",
       "      <td>3.542857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0007163932</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin  avg(overall)\n",
       "0  0002216973      4.833333\n",
       "1  0006476155      4.299419\n",
       "2  0006544150      4.222222\n",
       "3  0006550479      3.542857\n",
       "4  0007163932      4.500000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books = reviews.groupBy('asin').agg(avg('overall')).toPandas()\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookid = '0006476155'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status code: 200\n",
      "\n",
      "Response headers: {'Content-Type': 'text/html;charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Server': 'Server', 'Date': 'Tue, 20 Nov 2018 23:17:36 GMT', 'Strict-Transport-Security': 'max-age=47474747; includeSubDomains; preload', 'Vary': 'Accept-Encoding,User-Agent,X-Amzn-CDN-Cache', 'P3P': 'policyref=\"https://www.amazon.com/w3c/p3p.xml\",CP=\"CAO DSP LAW CUR ADM IVAo IVDo CONo OTPo OUR DELi PUBi OTRi BUS PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA HEA PRE LOC GOV OTC \"', 'Cache-Control': 'no-cache, no-transform', 'Content-Encoding': 'gzip', 'X-XSS-Protection': '1;', 'X-Content-Type-Options': 'nosniff', 'X-Frame-Options': 'SAMEORIGIN', 'x-amz-rid': '520JZ7Z5ZRSMKS0H6YZC', 'X-Cache': 'Miss from cloudfront', 'Via': '1.1 87e53d6d1b409d9ddfa1cf973907c0eb.cloudfront.net (CloudFront)', 'X-Amz-Cf-Id': '9SSruJ1WyX__OEVM6cxWi-b9GjqdCYjKQjZLSVk5zIniQnKWaHQuew=='}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make the request\n",
    "r = requests.get('https://www.amazon.com/dp/' + bookid)\n",
    "\n",
    "print('Response status code: {0}\\n'.format(r.status_code))\n",
    "print('Response headers: {0}\\n'.format(r.headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Along Came a Spider'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"span\", id='productTitle').string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book details\n",
    "book_info = []\n",
    "for li in soup.select('table#productDetailsTable div.content ul li'):\n",
    "    try:\n",
    "        title = li.b\n",
    "        key = title.text.strip().rstrip(':')\n",
    "        value = title.next_sibling.strip()\n",
    "        value = value.strip(\"()\")\n",
    "        book_info.append((key,value))\n",
    "    except AttributeError:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Paperback', '448 pages'),\n",
       " ('Publisher', 'HarperCollins; New Ed edition (March 2004'),\n",
       " ('Language', 'English'),\n",
       " ('ISBN-10', '9780006476153'),\n",
       " ('ISBN-13', '978-0006476153'),\n",
       " ('ASIN', '0006476155'),\n",
       " ('Product Dimensions', '4.3 x 1.5 x 6.8 inches'),\n",
       " ('Shipping Weight', '8.5 ounces'),\n",
       " ('Average Customer Review', ''),\n",
       " ('Amazon Best Sellers Rank', '#5,111,537 in Books ')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = soup.select('#histogramTable')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 star58%4 star25%3 star9%2 star4%1 star4%\n"
     ]
    }
   ],
   "source": [
    "for i in soup.select('#histogramTable'):\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5', '58'), ('4', '25'), ('3', '9'), ('2', '4'), ('1', '4')]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "rat = re.findall(u'(\\d) star(\\d+)%', ratings)\n",
    "rat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.75 µs ± 15.8 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "avg = 0\n",
    "for pair in rat:\n",
    "    avg += int(pair[0])*int(pair[1])/100\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_details_amazon(isbn):\n",
    "    \n",
    "    # Amazon Scraping\n",
    "    amazon_base_url = \"https://www.amazon.com/dp/\"\n",
    "    amazon_url = amazon_base_url + isbn\n",
    "#     req = Request(amazon_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "#     page = urlopen(req).read().decode(\"utf-8\")\n",
    "    req = requests.get(amazon_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    \n",
    "    # Book title\n",
    "    a_title = soup.find(\"span\", id='productTitle').string\n",
    "    \n",
    "    # Book details\n",
    "    book_info = []\n",
    "    for li in soup.select('table#productDetailsTable div.content ul li'):\n",
    "        try:\n",
    "            title = li.b\n",
    "            key = title.text.strip().rstrip(':')\n",
    "            value = title.next_sibling.strip()\n",
    "            value = value.strip(\"()\")\n",
    "            book_info.append((key,value))\n",
    "        except AttributeError:\n",
    "            break\n",
    "            \n",
    "    # Amazon reviews scraping\n",
    "    amazon_review_base_url = \"https://www.amazon.com/product-reviews/\"\n",
    "    amazon_review_url = amazon_review_base_url + isbn + \"/ref=cm_cr_getr_d_paging_btm_2?pageNumber=\"\n",
    "#     req = Request(amazon_review_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "#     page = urlopen(req).read().decode(\"utf-8\")\n",
    "#     soup = BeautifulSoup(page, 'html.parser')\n",
    "    req = requests.get(amazon_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'lxml')\n",
    "    \n",
    "    # List of book reviews in Amazon\n",
    "    reviews_list = []\n",
    "    reviews_list_final = []\n",
    "    for pg in range(1,5):\n",
    "        amazon_review_url = amazon_review_base_url + isbn + \"/ref=cm_cr_getr_d_paging_btm_2?pageNumber=\" + str(pg)\n",
    "        req = requests.get(amazon_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        page = req.text\n",
    "        soup = BeautifulSoup(page, 'lxml')\n",
    "        txt = soup.find(\"div\", id=\"cm_cr-review_list\")\n",
    "        try:\n",
    "            for rawreview in txt.find_all('span', {'class' : 'a-size-base review-text'}):\n",
    "                text = rawreview.parent.parent.parent.text\n",
    "                startindex = text.index('5 stars') + 7\n",
    "                endindex = text.index('Was this review helpful to you?')\n",
    "                text = text[startindex:endindex]\n",
    "                text = text.split(\"Verified Purchase\")[1]\n",
    "                rText = text.split(\".\")[:-1]\n",
    "                review_text = \"\"\n",
    "                for i in range(len(rText)):\n",
    "                    review_text += rText[i]\n",
    "                    review_text += \".\"\n",
    "                if review_text is not \"\":\n",
    "                    if \"|\" not in review_text:\n",
    "                        reviews_list.append(review_text)\n",
    "                    else:\n",
    "                        rText = text.split(\".\")[:-2]\n",
    "                        review_text = \"\"\n",
    "                        for x in range(len(rText)):\n",
    "                            review_text += rText[x]\n",
    "                            review_text += \".\"\n",
    "                        reviews_list.append(review_text)\n",
    "        except AttributeError:\n",
    "            review_text = \"No reviews found.\"\n",
    "    \n",
    "#     if amazon_reviews_count < len(reviews_list):\n",
    "#         reviews_list_final = reviews_list[:amazon_reviews_count]\n",
    "#     else:\n",
    "    reviews_list_final = reviews_list\n",
    "        \n",
    "    # Printing book details from Amazon\n",
    "    print('**Book Details from Amazon**')\n",
    "    #print(\"Book Details from Amazon\\n\")\n",
    "    print(\"Book Title: \",a_title)\n",
    "    #print(\"\\n\")\n",
    "    for i in range(len(book_info)):\n",
    "        print(f\"{book_info[i][0]} : {book_info[i][1]}\")\n",
    "        #print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    if len(reviews_list_final) == 0:\n",
    "        print(review_text)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(f\"Displaying top {amazon_reviews_count} book reviews:\\n\")\n",
    "        for i in range(len(reviews_list_final)):\n",
    "            review_txt_list = reviews_list_final[i].split(\".\")[:3]\n",
    "            review_txt = \"\"\n",
    "            for j in range(len(review_txt_list)):\n",
    "                review_txt += review_txt_list[j]\n",
    "                review_txt += \".\"\n",
    "            review_txt += \"..\"\n",
    "            print(review_txt)\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Book Details from Amazon**\n",
      "Book Title:  Along Came a Spider\n",
      "Paperback : 448 pages\n",
      "Publisher : HarperCollins; New Ed edition (March 2004\n",
      "Language : English\n",
      "ISBN-10 : 9780006476153\n",
      "ISBN-13 : 978-0006476153\n",
      "ASIN : 0006476155\n",
      "Product Dimensions : 4.3 x 1.5 x 6.8 inches\n",
      "Shipping Weight : 8.5 ounces\n",
      "Average Customer Review : \n",
      "Amazon Best Sellers Rank : #5,111,537 in Books \n",
      "\n",
      "\n",
      "No reviews found.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book_details_amazon(bookid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
