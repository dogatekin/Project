{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Data from Amazon\n",
    "---\n",
    "\n",
    "The dataset we want:\n",
    "\n",
    "| ID | Review Score | Sales Rank | Category    | Title | Author | Date    | Visual Features     |\n",
    "| -- | ------------ | ---------- | ----------- | ----- | ------ | ------- | ------------------- |\n",
    "\n",
    "The dataset we have, as downloaded from [here](https://github.com/uchidalab/book-dataset):\n",
    "\n",
    "| ID | Filename | Image URL | Title | Author | Category ID | Category |\n",
    "| -- | -------- | --------- | ----- | ------ | ----------- | -------- |\n",
    "\n",
    "The `ID` column in the data can be used to access the webpage of each book, by connecting to https://www.amazon.com/dp/book-id. This allows us to scrape any data that is missing directly from Amazon.\n",
    "\n",
    "We already have the Title, Author and Category of each book ready to be used.\n",
    "\n",
    "For everything else, there's ~~Mastercard~~ BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:stem:GETCONF __owningcontrollerprocess (runtime: 0.0003)\n",
      "DEBUG:stem:RESETCONF __OwningControllerProcess (runtime: 0.0059)\n"
     ]
    }
   ],
   "source": [
    "# To request data from Amazon\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# To open image links\n",
    "import urllib\n",
    "\n",
    "# To process data\n",
    "import pandas as pd\n",
    "\n",
    "# To extract information from weirdly formatted Amazon info\n",
    "import re\n",
    "\n",
    "# To create random delays to trick the Amazon bot detector\n",
    "from time import sleep\n",
    "import random\n",
    "\n",
    "# To rotate IPs while scraping\n",
    "from torrequest import TorRequest\n",
    "tor = TorRequest(password='ilovecs401')\n",
    "\n",
    "# To read data\n",
    "import csv\n",
    "\n",
    "# To check if a file is downloaded already\n",
    "import os\n",
    "\n",
    "# To print an image in the notebook programmatically\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Set data directories\n",
    "ORIGINAL_DATA_DIR = 'Original Data/'\n",
    "COLLECTED_DATA_DIR = 'Collected Data/'\n",
    "IMAGE_DIR = COLLECTED_DATA_DIR + 'Cover Images/'\n",
    "HTML_DIR = '/Users/dogatekin/Data/HTML Files/'\n",
    "\n",
    "# Constants\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux i686; rv:64.0) Gecko/20100101 Firefox/64.0',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:64.0) Gecko/20100101 Firefox/64.0',\n",
    "    'Opera/9.80 (X11; Linux i686; Ubuntu/14.10) Presto/2.12.388 Version/12.16',\n",
    "    'Opera/9.80 (Macintosh; Intel Mac OS X 10.14.1) Presto/2.12.388 Version/12.16'\n",
    "]\n",
    "DEFAULT_USER_AGENT = USER_AGENTS[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Original Data\n",
    "---\n",
    "\n",
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Image URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Category ID</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>761183272</td>\n",
       "      <td>0761183272.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61Y5cOdH...</td>\n",
       "      <td>Mom's Family Wall Calendar 2016</td>\n",
       "      <td>Sandra Boynton</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1623439671</td>\n",
       "      <td>1623439671.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61t-hrSw...</td>\n",
       "      <td>Doug the Pug 2016 Wall Calendar</td>\n",
       "      <td>Doug the Pug</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00O80WC6I</td>\n",
       "      <td>B00O80WC6I.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/41X-KQqs...</td>\n",
       "      <td>Moleskine 2016 Weekly Notebook, 12M, Large, Bl...</td>\n",
       "      <td>Moleskine</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>761182187</td>\n",
       "      <td>0761182187.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61j-4gxJ...</td>\n",
       "      <td>365 Cats Color Page-A-Day Calendar 2016</td>\n",
       "      <td>Workman Publishing</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1578052084</td>\n",
       "      <td>1578052084.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51Ry4Tsq...</td>\n",
       "      <td>Sierra Club Engagement Calendar 2016</td>\n",
       "      <td>Sierra Club</td>\n",
       "      <td>3</td>\n",
       "      <td>Calendars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID        Filename  \\\n",
       "0   761183272  0761183272.jpg   \n",
       "1  1623439671  1623439671.jpg   \n",
       "2  B00O80WC6I  B00O80WC6I.jpg   \n",
       "3   761182187  0761182187.jpg   \n",
       "4  1578052084  1578052084.jpg   \n",
       "\n",
       "                                           Image URL  \\\n",
       "0  http://ecx.images-amazon.com/images/I/61Y5cOdH...   \n",
       "1  http://ecx.images-amazon.com/images/I/61t-hrSw...   \n",
       "2  http://ecx.images-amazon.com/images/I/41X-KQqs...   \n",
       "3  http://ecx.images-amazon.com/images/I/61j-4gxJ...   \n",
       "4  http://ecx.images-amazon.com/images/I/51Ry4Tsq...   \n",
       "\n",
       "                                               Title              Author  \\\n",
       "0                    Mom's Family Wall Calendar 2016      Sandra Boynton   \n",
       "1                    Doug the Pug 2016 Wall Calendar        Doug the Pug   \n",
       "2  Moleskine 2016 Weekly Notebook, 12M, Large, Bl...           Moleskine   \n",
       "3            365 Cats Color Page-A-Day Calendar 2016  Workman Publishing   \n",
       "4               Sierra Club Engagement Calendar 2016         Sierra Club   \n",
       "\n",
       "   Category ID   Category  \n",
       "0            3  Calendars  \n",
       "1            3  Calendars  \n",
       "2            3  Calendars  \n",
       "3            3  Calendars  \n",
       "4            3  Calendars  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_names = ['ID', 'Filename', 'Image URL', 'Title', 'Author', 'Category ID', 'Category']\n",
    "\n",
    "books = pd.read_csv(ORIGINAL_DATA_DIR + 'book32-listing.csv', encoding='latin1', header=None, names=header_names)\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calendars\n",
      "Comics & Graphic Novels\n",
      "Test Preparation\n",
      "Mystery, Thriller & Suspense\n",
      "Science Fiction & Fantasy\n",
      "Romance\n",
      "Humor & Entertainment\n",
      "Literature & Fiction\n",
      "Gay & Lesbian\n",
      "Engineering & Transportation\n",
      "Cookbooks, Food & Wine\n",
      "Crafts, Hobbies & Home\n",
      "Arts & Photography\n",
      "Education & Teaching\n",
      "Parenting & Relationships\n",
      "Self-Help\n",
      "Computers & Technology\n",
      "Medical Books\n",
      "Science & Math\n",
      "Health, Fitness & Dieting\n",
      "Business & Money\n",
      "Law\n",
      "Biographies & Memoirs\n",
      "History\n",
      "Politics & Social Sciences\n",
      "Reference\n",
      "Christian Books & Bibles\n",
      "Religion & Spirituality\n",
      "Sports & Outdoors\n",
      "Teen & Young Adult\n",
      "Children's Books\n",
      "Travel\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(books['Category'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want the Children's Books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Filename</th>\n",
       "      <th>Image URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>545790352</td>\n",
       "      <td>0545790352.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51MIi4p2...</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone: The Ill...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1419717014</td>\n",
       "      <td>1419717014.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61YgGsg-...</td>\n",
       "      <td>Diary of a Wimpy Kid: Old School</td>\n",
       "      <td>Jeff Kinney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1423160916</td>\n",
       "      <td>1423160916.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/611CmvkL...</td>\n",
       "      <td>Magnus Chase and the Gods of Asgard, Book 1: T...</td>\n",
       "      <td>Rick Riordan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1476789886</td>\n",
       "      <td>1476789886.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51KqU7Dw...</td>\n",
       "      <td>Rush Revere and the Star-Spangled Banner</td>\n",
       "      <td>Rush Limbaugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1338029991</td>\n",
       "      <td>1338029991.jpg</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61kvq74k...</td>\n",
       "      <td>Harry Potter Coloring Book</td>\n",
       "      <td>Scholastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID        Filename  \\\n",
       "0   545790352  0545790352.jpg   \n",
       "1  1419717014  1419717014.jpg   \n",
       "2  1423160916  1423160916.jpg   \n",
       "3  1476789886  1476789886.jpg   \n",
       "4  1338029991  1338029991.jpg   \n",
       "\n",
       "                                           Image URL  \\\n",
       "0  http://ecx.images-amazon.com/images/I/51MIi4p2...   \n",
       "1  http://ecx.images-amazon.com/images/I/61YgGsg-...   \n",
       "2  http://ecx.images-amazon.com/images/I/611CmvkL...   \n",
       "3  http://ecx.images-amazon.com/images/I/51KqU7Dw...   \n",
       "4  http://ecx.images-amazon.com/images/I/61kvq74k...   \n",
       "\n",
       "                                               Title         Author  \n",
       "0  Harry Potter and the Sorcerer's Stone: The Ill...   J.K. Rowling  \n",
       "1                   Diary of a Wimpy Kid: Old School    Jeff Kinney  \n",
       "2  Magnus Chase and the Gods of Asgard, Book 1: T...   Rick Riordan  \n",
       "3           Rush Revere and the Star-Spangled Banner  Rush Limbaugh  \n",
       "4                         Harry Potter Coloring Book     Scholastic  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books = books[books['Category'] == \"Children's Books\"].reset_index(drop=True)\n",
    "# We don't need the Category or Category ID columns anymore\n",
    "books.drop(columns=['Category ID', 'Category'], inplace=True)\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many books we have left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13605"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's fix the IDs in the dataset. For some reason, the ID column has the leading 0s removed (normally all of them should be 10 characters long), which makes the webpages inaccessible. The filename column has the correct IDs with the correct number of leading 0s. So let's use the Filename column as the new ID column, we can add the `.jpg` extension later when downloading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Image URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0545790352</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51MIi4p2...</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone: The Ill...</td>\n",
       "      <td>J.K. Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1419717014</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61YgGsg-...</td>\n",
       "      <td>Diary of a Wimpy Kid: Old School</td>\n",
       "      <td>Jeff Kinney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1423160916</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/611CmvkL...</td>\n",
       "      <td>Magnus Chase and the Gods of Asgard, Book 1: T...</td>\n",
       "      <td>Rick Riordan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1476789886</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/51KqU7Dw...</td>\n",
       "      <td>Rush Revere and the Star-Spangled Banner</td>\n",
       "      <td>Rush Limbaugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1338029991</td>\n",
       "      <td>http://ecx.images-amazon.com/images/I/61kvq74k...</td>\n",
       "      <td>Harry Potter Coloring Book</td>\n",
       "      <td>Scholastic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                          Image URL  \\\n",
       "0  0545790352  http://ecx.images-amazon.com/images/I/51MIi4p2...   \n",
       "1  1419717014  http://ecx.images-amazon.com/images/I/61YgGsg-...   \n",
       "2  1423160916  http://ecx.images-amazon.com/images/I/611CmvkL...   \n",
       "3  1476789886  http://ecx.images-amazon.com/images/I/51KqU7Dw...   \n",
       "4  1338029991  http://ecx.images-amazon.com/images/I/61kvq74k...   \n",
       "\n",
       "                                               Title         Author  \n",
       "0  Harry Potter and the Sorcerer's Stone: The Ill...   J.K. Rowling  \n",
       "1                   Diary of a Wimpy Kid: Old School    Jeff Kinney  \n",
       "2  Magnus Chase and the Gods of Asgard, Book 1: T...   Rick Riordan  \n",
       "3           Rush Revere and the Star-Spangled Banner  Rush Limbaugh  \n",
       "4                         Harry Potter Coloring Book     Scholastic  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books['ID'] = books['Filename'].apply(lambda row: re.findall(u'(.*).jpg', row)[0])\n",
    "books.drop(columns='Filename', inplace=True)\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping New Data\n",
    "---\n",
    "\n",
    "The columns we need to scrape are: `Review Score`, `Sales Rank` and `Date`. We also need to download the images from the URLs so that we can extract visual features from them, completing our dataset. Just in case we need some other information in the future from the webpages, we will also save the raw HTML files so we don't have to scrape them from Amazon again.\n",
    "\n",
    "First we will demonstrate the scraping process for each column on an arbitrary example, then we will combine these in a function and scrape the information for all the books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                                  0545790352\n",
       "Image URL    http://ecx.images-amazon.com/images/I/51MIi4p2...\n",
       "Title        Harry Potter and the Sorcerer's Stone: The Ill...\n",
       "Author                                            J.K. Rowling\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_book = books.iloc[0]\n",
    "example_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Amazon\n",
    "\n",
    "This step is trickier than it sounds. Sending many requests to Amazon servers in quick succession always leads to Captcha pages that check if the request came from a human. In this case, it is indeed not coming from a human so we need to be smarter. We use Tor requests to be able to change our IP at any time and also rotate the User Agent we use to send the request.\n",
    "\n",
    "We also noticed that at least one Tor IP was unable to connect to the servers, so we try the initial request many times with different IPs and user agents until we get a response without any connection errors or getting caught by the bot detector. When a request is successful, we keep using the found IP-agent pair until it fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(book_id, agent=DEFAULT_USER_AGENT, max_tries=10):\n",
    "    for _ in range(max_tries):\n",
    "        try:\n",
    "            # Creating random delays before requests helps to avoid detection\n",
    "            sleep(random.randint(1, 3))\n",
    "            \n",
    "            # Try to connect\n",
    "            response = tor.get('https://www.amazon.com/dp/' + book_id, headers={'User-Agent': agent})\n",
    "            \n",
    "            # Make soup if we didn't get a connection error\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "            \n",
    "            # If we get redirected to a Captcha page raise error to try again\n",
    "            if(soup.title.string == 'Robot Check'):\n",
    "                raise ConnectionError\n",
    "            \n",
    "            # If we successfully reach the webpage, return the soup, successful agent and raw HTML\n",
    "            return soup, agent, response.text\n",
    "        \n",
    "        except ConnectionError:\n",
    "            # If something is wrong with the IP, get a new IP and user agent and try again\n",
    "            tor.reset_identity()\n",
    "            agent = random.choice(USER_AGENTS)\n",
    "    \n",
    "    raise ConnectionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it on our example book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Harry Potter and the Sorcerer's Stone: The Illustrated Edition (Harry Potter, Book 1): J.K. Rowling, Jim Kay: 9780545790352: Amazon.com: Books\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup, _, _ = connect(example_book['ID'])\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales Rank and Date\n",
    "\n",
    "We can get both of these from the product details table on the webpage, which is in a table conveniently named `productDetailsTable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<b>Age Range:</b>,\n",
       " <b>Grade Level:</b>,\n",
       " <b>Series:</b>,\n",
       " <b>Hardcover:</b>,\n",
       " <b>Publisher:</b>,\n",
       " <b>Language:</b>,\n",
       " <b>ISBN-10:</b>,\n",
       " <b>ISBN-13:</b>,\n",
       " <b>\n",
       "     Product Dimensions: \n",
       "     </b>,\n",
       " <b>Shipping Weight:</b>,\n",
       " <b>Average Customer Review:</b>,\n",
       " <b>Amazon Best Sellers Rank:</b>,\n",
       " <b><a href=\"https://www.amazon.com/gp/bestsellers/books/3153/ref=pd_zg_hrsr_books_1_5_last/134-2712085-9861750\">Friendship</a></b>,\n",
       " <b><a href=\"https://www.amazon.com/gp/bestsellers/books/2967/ref=pd_zg_hrsr_books_2_3_last/134-2712085-9861750\">Action &amp; Adventure</a></b>,\n",
       " <b><a href=\"https://www.amazon.com/gp/bestsellers/books/3017/ref=pd_zg_hrsr_books_3_4_last/134-2712085-9861750\">Fantasy &amp; Magic</a></b>]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('#productDetailsTable li b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use regex to extract the info we need from the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales Rank: 124\n",
      "Date: October 6, 2015\n"
     ]
    }
   ],
   "source": [
    "for li in soup.select('#productDetailsTable li'):\n",
    "    # We only need two of the list items\n",
    "    if(li.b.string == 'Amazon Best Sellers Rank:'):\n",
    "        # The rank is given in the format #1,234,567\n",
    "        sales_rank = re.findall(u'#([\\d,]+)', li.b.nextSibling)[0]\n",
    "    elif(li.b.string == 'Publisher:'):\n",
    "        # The date is in the last set of parantheses\n",
    "        date = re.findall(u'\\(([^\\(\\)]*)\\)$', li.b.nextSibling)[0]\n",
    "        \n",
    "print(f'Sales Rank: {sales_rank}\\nDate: {date}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn it into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rank_date(soup):\n",
    "    for li in soup.select('#productDetailsTable li'):\n",
    "        if(li.b.string == 'Amazon Best Sellers Rank:'):\n",
    "            try:\n",
    "                sales_rank = re.findall(u'#([\\d,]+)', li.b.nextSibling)[0]  # Format: #1,234,567\n",
    "                sales_rank = int(sales_rank.replace(',',''))  # Remove the commas and convert to integer\n",
    "            except:\n",
    "                sales_rank = None  # couldn't scrape\n",
    "        elif(li.b.string == 'Publisher:'):\n",
    "            try:\n",
    "                date = re.findall(u'\\(([^\\(\\)]*)\\)$', li.b.nextSibling)[0]  # Format: Inside last parantheses\n",
    "            except:\n",
    "                date = None  # couldn't scrape\n",
    "                \n",
    "    return sales_rank, date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try on example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 'October 6, 2015')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_rank_date(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Score\n",
    "\n",
    "You might have noticed there is also an item called `Average Customer Review` in the table we just used to extract the Rank and Date. Inside that item, all the review scores are found in a table with the id `histogramTable`, that gives the percentages of users for each score from 1 to 5 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5 star87%4 star8%3 star2%2 star1%1 star2%'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = soup.select('#histogramTable')[0].text\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formatting is not great, but it's nothing we can't fix by using a simple regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5', '87'), ('4', '8'), ('3', '2'), ('2', '1'), ('1', '2')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = re.findall(u'(\\d) star(\\d+)%', reviews)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted average of these scores is our final Review Score for the given book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.77"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = 0\n",
    "for pair in reviews:\n",
    "    score += int(pair[0]) * int(pair[1])/100  # weights are percentages\n",
    "\n",
    "round(score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(soup):\n",
    "    try:\n",
    "        reviews = soup.select('#histogramTable')[0].text\n",
    "        reviews = re.findall(u'(\\d) star(\\d+)%', reviews)\n",
    "\n",
    "        score = 0\n",
    "        for pair in reviews:\n",
    "            score += int(pair[0]) * int(pair[1])/100  # weights are percentages\n",
    "\n",
    "        score = round(score, 3)\n",
    "    except:\n",
    "        score = None  # couldn't scrape\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try on example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.77"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_score(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cover Image\n",
    "\n",
    "The image URL of each book is available in the original dataset, let's make a HashMap of `ID:URL` pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0545790352': 'http://ecx.images-amazon.com/images/I/51MIi4p2YyL.jpg',\n",
       " '1419717014': 'http://ecx.images-amazon.com/images/I/61YgGsg-k-L.jpg',\n",
       " '1423160916': 'http://ecx.images-amazon.com/images/I/611CmvkLO4L.jpg',\n",
       " '1476789886': 'http://ecx.images-amazon.com/images/I/51KqU7Dw9SL.jpg',\n",
       " '1338029991': 'http://ecx.images-amazon.com/images/I/61kvq74kVSL.jpg'}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = books[['ID', 'Image URL']].set_index('ID').to_dict()['Image URL']\n",
    "\n",
    "# Show random 5 mappings\n",
    "dict(list(urls.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it on our example book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://ecx.images-amazon.com/images/I/51MIi4p2YyL.jpg'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_url = urls[example_book['ID']]\n",
    "example_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![Example Image](http://ecx.images-amazon.com/images/I/51MIi4p2YyL.jpg)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(f'![Example Image]({example_url})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn it into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(book_id):\n",
    "    url = urls[book_id]\n",
    "    filename = book_id + '.jpg'\n",
    "    \n",
    "    # Download only if not already downloaded\n",
    "    if not os.path.isfile(IMAGE_DIR + filename):\n",
    "        downloaded_img = urllib.request.urlopen(url)\n",
    "        f = open(IMAGE_DIR + filename, mode='wb')\n",
    "        f.write(downloaded_img.read())\n",
    "        downloaded_img.close()\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw HTML\n",
    "\n",
    "Save the raw HTML files so we don't have to scrape them from Amazon again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(book_id, html_text):\n",
    "    filename = book_id + '.html'\n",
    "    \n",
    "    # Save only if not already saved\n",
    "    if not os.path.isfile(HTML_DIR + filename):\n",
    "        html_file = open(HTML_DIR + filename,\"w\")\n",
    "        html_file.write(html_text)\n",
    "        html_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it together\n",
    "\n",
    "Let's bring all of the functions we created together under one function that will connect to the webpage, scrape all the necessary info, download the cover image and save the HTML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_info(book_id, agent=DEFAULT_USER_AGENT):\n",
    "    try:\n",
    "        # Connect to Amazon, keep track of agent\n",
    "        soup, current_agent, raw_html = connect(book_id, agent)\n",
    "\n",
    "        # Save the HTML file\n",
    "        save_html(book_id, raw_html)\n",
    "\n",
    "        # Get sales rank and date\n",
    "        sales_rank, date = extract_rank_date(soup)\n",
    "\n",
    "        # Get average review score\n",
    "        score = extract_score(soup)\n",
    "\n",
    "        # Download cover image\n",
    "        download_image(book_id)\n",
    "        \n",
    "    except ConnectionError:\n",
    "        current_agent = agent\n",
    "        sales_rank = date = score = None\n",
    "        \n",
    "    return current_agent, book_id, sales_rank, date, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a final test on the example book we used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0545790352', 124, 'October 6, 2015', 4.77)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped = scrape_info(example_book['ID'])\n",
    "scraped[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing the dataset\n",
    "---\n",
    "\n",
    "To be able to stop and continue at will, we will write the scraped info to a csv file as we go along, and simultaneously download cover images. Let's initialize this file with a meaningful header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(COLLECTED_DATA_DIR + 'scraped.csv', 'a') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['ID', 'Sales Rank', 'Date', 'Review Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go through the dataset, starting scraping from where we last left off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping stopped by manual interruption. Check the last downloaded book cover image and the last row of the CSV file to make sure there were no corruptions. Total number of books scraped until interruption: 2.\n"
     ]
    }
   ],
   "source": [
    "with open(COLLECTED_DATA_DIR + 'scraped.csv', 'a+') as file:\n",
    "    reader = csv.reader(file)\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Look at the last scraped book to continue from the next one in the dataset\n",
    "    file.seek(0)\n",
    "    last_scraped = next(reversed(list(reader)))[0]\n",
    "    \n",
    "    if(last_scraped == 'ID'):\n",
    "        # Nothing was scraped yet, start from the beginning\n",
    "        index = 0\n",
    "    else:\n",
    "        # At least one book was scraped, find the index of the last scraped book and start from the next one\n",
    "        last_scraped_index = books.index[books['ID'] == last_scraped].tolist()[0]\n",
    "        index = last_scraped_index + 1\n",
    "     \n",
    "    # Initialize user agent and scrape counter\n",
    "    agent = DEFAULT_USER_AGENT\n",
    "    count = 0\n",
    "    \n",
    "    try:\n",
    "        # Until every book is scraped\n",
    "        while(index < books.shape[0]):\n",
    "            # Get the book at current index\n",
    "            current_id = books.iloc[index]['ID']\n",
    "\n",
    "            # Scrape everything\n",
    "            scraped = scrape_info(current_id, agent)\n",
    "\n",
    "            # Keep track of agent\n",
    "            agent = scraped[0]\n",
    "\n",
    "            # Write to CSV\n",
    "            writer.writerow(scraped[1:])\n",
    "            file.flush()\n",
    "\n",
    "            # Move on to next book and increment counter\n",
    "            index += 1\n",
    "            count += 1\n",
    "\n",
    "            # Print info about scraping process\n",
    "            print(f'Number of scraped books: {count}', end='\\r')\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(f'Scraping stopped by manual interruption. Check the last downloaded book cover image and the last row of the CSV file to make sure there were no corruptions. Total number of books scraped until interruption: {count}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Collected Data\n",
    "---\n",
    "\n",
    "Now that we have the data collected, we should make sure it's clean before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
